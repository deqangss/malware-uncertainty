from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np
import tensorflow_probability as tfp

import os
import sys
import shutil
import hashlib
import warnings
import functools


class ParamWrapper(object):
    def __init__(self, params):
        if not isinstance(params, dict):
            params = vars(params)
        self.params = params

    def __getattr__(self, name):
        val = self.params.get(name)
        if val is None:
            MSG = "Setting params ({}) is deprecated"
            warnings.warn(MSG.format(name))
        return val


def retrive_files_set(base_dir, dir_ext, file_ext):
    """
    get file paths given the directory
    :param base_dir: basic directory
    :param dir_ext: directory append at the rear of base_dir
    :param file_ext: file extension
    :return: set of file paths. Avoid the repetition
    """

    def get_file_name(root_dir, file_ext):

        for dir_path, dir_names, file_names in os.walk(root_dir, topdown=True):
            for file_name in file_names:
                _ext = file_ext
                if os.path.splitext(file_name)[1] == _ext:
                    yield os.path.join(dir_path, file_name)
                elif '.' not in file_ext:
                    _ext = '.' + _ext

                    if os.path.splitext(file_name)[1] == _ext:
                        yield os.path.join(dir_path, file_name)
                    else:
                        pass
                else:
                    pass

    if file_ext is not None:
        file_exts = file_ext.split("|")
    else:
        file_exts = ['']
    file_path_set = set()
    for ext in file_exts:
        file_path_set = file_path_set | set(get_file_name(os.path.join(base_dir, dir_ext), ext))

    return list(file_path_set)


def dump_pickle(data, path):
    try:
        import pickle as pkl
    except Exception as e:
        import cPickle as pkl

    if not os.path.exists(os.path.dirname(path)):
        mkdir(os.path.dirname(path))
    with open(path, 'wb') as wr:
        pkl.dump(data, wr)
    return True


def read_pickle(path):
    try:
        import pickle as pkl
    except Exception as e:
        import cPickle as pkl

    if os.path.isfile(path):
        with open(path, 'rb') as fr:
            return pkl.load(fr)
    else:
        raise IOError("The {0} is not been found.".format(path))


def dump_joblib(data, path):
    if not os.path.exists(os.path.dirname(path)):
        mkdir(os.path.dirname(path))

    try:
        import joblib
        with open(path, 'wb') as wr:
            joblib.dump(data, wr)
    except IOError:
        raise IOError("Dump data failed.")


def read_joblib(path):
    import joblib
    if os.path.isfile(path):
        with open(path, 'rb') as fr:
            return joblib.load(fr)
    else:
        raise IOError("The {0} is not a file.".format(path))


def read_txt(path, mode='r'):
    if os.path.isfile(path):
        with open(path, mode) as f_r:
            lines = f_r.read().strip().splitlines()
            return lines
    else:
        raise ValueError("{} does not seen like a file path.\n".format(path))


def dump_txt(data_str, path, mode='w'):
    if not isinstance(data_str, str):
        raise TypeError

    with open(path, mode) as f_w:
        f_w.write(data_str)


def readdata_np(data_path):
    try:
        with open(data_path, 'rb') as f_r:
            data = np.load(f_r)
        return data
    except IOError as e:
        raise IOError("Unable to open {0}: {1}.\n".format(data_path, str(e)))


def dumpdata_np(data, data_path):
    if not isinstance(data, np.ndarray):
        warnings.warn("The array is not the numpy.ndarray type.")
    data_dir = os.path.dirname(data_path)
    try:
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
        with open(data_path, 'wb') as f_s:
            np.save(f_s, data)
    except OSError as e:
        sys.stderr.write(e)


def load_json(json_path):
    try:
        import yaml
        with open(json_path, 'r') as rh:
            return yaml.safe_load(rh)
    except IOError as ex:
        raise IOError(str(ex) + ": Unable to load json file.")


def dump_json(obj_dict, file_path):
    try:
        import json
        if not os.path.exists(os.path.dirname(file_path)):
            mkdir(os.path.dirname(file_path))

        with open(file_path, 'w') as fh:
            json.dump(obj_dict, fh)
    except IOError as ex:
        raise IOError(str(ex) + ": Fail to dump dict using json toolbox")


def mkdir(target):
    try:
        if os.path.isfile(target):
            target = os.path.dirname(target)

        if not os.path.exists(target):
            os.makedirs(target)
        return 0
    except IOError as e:
        raise Exception("Fail to create directory! Error:" + str(e))


def copy_files(src_file_list, dst_dir):
    if not isinstance(src_file_list, list):
        raise TypeError
    if os.path.isdir(dst_dir):
        raise ValueError
    for src in src_file_list:
        if not os.path.isfile(src):
            continue
        shutil.copy(src, dst_dir)


def get_sha256(file_path):
    assert os.path.isfile(file_path), 'permit only file path'
    fh = open(file_path, 'rb')
    sha256 = hashlib.sha256()
    while True:
        data = fh.read(8192)
        if not data:
            break
        sha256.update(data)
    return sha256.hexdigest()


def merge_namedtuples(tp1, tp2):
    from collections import namedtuple
    _TP12 = namedtuple('tp12', tp1._fileds + tp2._fields)
    return _TP12(*(tp1 + tp2))


########################################################################################
############################# functions for tf models ##################################
########################################################################################
ensemble_method_scopes = ['vanilla', 'mc_dropout', 'deep_ensemble', 'weighted_ensemble', 'anchor', 'bayesian']


class DenseDropout(tf.keras.layers.Layer):
    def __init__(self, units,
                 dropout_rate,
                 activation=None,
                 use_dropout=True,
                 **kwargs):
        """
        Initialize a dense-dropout layer
        :param units: number of neurons
        :param dropout_rate: a float value between 0 and 1. A portion of activations will be dropped randomly
        :param activation: activation function
        param use_dropout: performing dropout in both training and testing phases
        :param kwargs: other arguments for tf.keras.layers.Dense
        """
        super(DenseDropout, self).__init__()
        self.units = units
        self.activation = activation
        self.dropout_rate = dropout_rate
        self.use_dropout = use_dropout
        self.kwargs = kwargs
        self.dense_layer = tf.keras.layers.Dense(units, activation=self.activation, **self.kwargs)
        self.dropout_layer = tf.keras.layers.Dropout(rate=dropout_rate)

    def call(self, inputs, training=True):
        return self.dropout_layer(self.dense_layer(inputs), training=self.use_dropout)


class Conv2DDropout(tf.keras.layers.Layer):
    def __init__(self,
                 filters,
                 kernel_size,
                 dropout_rate,
                 activation=None,
                 use_dropout=True,
                 **kwargs):
        """
        Initialize a convolution-dropout layer
        :param filters: Positive integer, number of ouput channels
        :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of 2D convolution window
        :param dropout_rate: a float value between 0 and 1. A portion of activations will be dropped randomly
        :param activation: activation function
        :param use_dropout: performing dropout in both training and testing phases
        :param kwargs: other arguments for tf.keras.layers.Conv2D
        """
        super(Conv2DDropout, self).__init__()
        self.filters = filters
        self.kernel_size = kernel_size
        self.dropout_rate = dropout_rate
        self.use_dropout = use_dropout
        self.conv2d_layer = tf.keras.layers.Conv2D(filters, kernel_size, activation=activation, **kwargs)
        self.dropout_layer = tf.keras.layers.Dropout(rate=dropout_rate)

    def call(self, inputs, training=True):
        return self.dropout_layer(self.conv2d_layer(inputs), training=self.use_dropout)


class LSTMDropout(tf.keras.layers.Layer):
    def __init__(self,
                 units,
                 dropout_rate,
                 use_dropout=True,
                 go_backwards=True,
                 return_sequences=True, **kwargs):
        """
        Initialize a LSTM-dropout layer
        :param dropout_rate: a float value between 0 and 1. A portion of activations will be dropped randomly
        :param units: Positive Integer, number of neurons
        :param use_dropout: performing dropout in both training and testing phases
        :param kwargs: other arguments for tf.keras.layers.LSTM
        """
        super(LSTMDropout, self).__init__()
        self.units = units
        self.dropout_rate = dropout_rate
        self.use_dropout = use_dropout
        self.go_backwards = go_backwards
        self.return_sequences = return_sequences
        self.lstm = tf.keras.layers.LSTM(units, dropout=self.dropout_rate, return_sequences=self.return_sequences,
                                         **kwargs)

    def call(self, inputs, training=True):
        return self.lstm(inputs, training=self.use_dropout)

    @property
    def return_state(self):
        return self.lstm.return_state

    def get_config(self):
        config = super(LSTMDropout, self).get_config()
        config['dropout_rate'] = self.dropout_rate
        config['units'] = self.units
        config['use_dropout'] = self.use_dropout
        config['go_backwards'] = self.go_backwards
        return config


class DropoutDense(tf.keras.layers.Layer):
    def __init__(self, units,
                 dropout_rate,
                 activation=None,
                 use_dropout=True,
                 **kwargs):
        """
        Initialize a dense-dropout layer
        :param units: number of neurons
        :param dropout_rate: a float value between 0 and 1. A portion of activations will be dropped randomly
        :param activation: activation function
        param use_dropout: performing dropout in both training and testing phases
        :param kwargs: other arguments for tf.keras.layers.Dense
        """
        super(DropoutDense, self).__init__()
        self.units = units
        self.activation = activation
        self.dropout_rate = dropout_rate
        self.use_dropout = use_dropout
        self.kwargs = kwargs
        self.dense_layer = tf.keras.layers.Dense(units, activation=self.activation, **self.kwargs)
        self.dropout_layer = tf.keras.layers.Dropout(rate=dropout_rate)

    def call(self, inputs, training=True):
        return self.dense_layer(self.dropout_layer(inputs, training=self.use_dropout))


class AnchorInitilizer(tf.keras.initializers.Initializer):
    def __init__(self, scale=2., seed=0):
        self.seed = seed
        self.scale = scale

    def __call__(self, shape, dtype=None):
        assert len(shape) >= 2
        n_in, n_out = get_fans(shape)
        self.var = tf.cast(self.scale, tf.float32) / tf.cast(n_in, tf.float32)
        tf.random.set_seed(self.seed)
        return tf.random.normal(shape=shape, mean=0., stddev=tf.sqrt(self.var))

    def get_config(self):
        return {'scale': self.scale, 'seed': self.seed}


# @tf.keras.utils.register_keras_serializable(package='Custom', name='anchor')
class AnchorRegularizer(tf.keras.regularizers.Regularizer):
    def __init__(self, scale=2.0, n_train=128, seed=0):
        self.scale = scale
        self.n_train = n_train
        self.seed = seed

    def __call__(self, x):
        shape = x.get_shape()
        n_in, n_out = get_fans(shape)
        var = tf.cast(self.scale, tf.float32) / tf.cast(n_in, tf.float32)
        _lambda = 1. / (2. * var)
        tf.random.set_seed(self.seed)
        init_weights = tf.random.normal(shape=shape, mean=0., stddev=tf.sqrt(var))
        return tf.reduce_sum(tf.square(x - init_weights)) * _lambda / tf.cast(self.n_train, tf.float32)

    def get_config(self):
        return {'scale': float(self.scale),
                'n_train': float(self.n_train),
                'seed': int(self.seed)}


def dense_dropout(dropout_rate=0.4):
    return functools.partial(DenseDropout, dropout_rate=dropout_rate)


def conv2d_dropout(dropout_rate=0.4):
    return functools.partial(Conv2DDropout, dropout_rate=dropout_rate)


def lstm_dropout(dropout_rate=0.4):
    return functools.partial(LSTMDropout, dropout_rate=dropout_rate)


def dropout_dense(dropout_rate=0.4):
    return functools.partial(DropoutDense, dropout_rate=dropout_rate)


def dense_anchor(scale=2., n_train=128, seed=0):
    return functools.partial(tf.keras.layers.Dense, kernel_initializer=AnchorInitilizer(scale=scale, seed=seed),
                             kernel_regularizer=AnchorRegularizer(scale=scale, n_train=n_train, seed=seed)
                             )


def conv2d_anchor(scale=2., n_train=128, seed=0):
    return functools.partial(tf.keras.layers.Conv2D, kernel_initializer=AnchorInitilizer(scale=scale, seed=seed),
                             kernel_regularizer=AnchorRegularizer(scale=scale, n_train=n_train, seed=seed)
                             )


def produce_layer(ensemble_type=None, **kwargs):
    assert ensemble_type in ensemble_method_scopes, 'only support ensemble method {}.'.format(
        ','.join(ensemble_method_scopes)
    )
    if ensemble_type == 'vanilla' or ensemble_type == 'deep_ensemble' or ensemble_type == 'weighted_ensemble':
        Dense = tf.keras.layers.Dense
        Conv2D = tf.keras.layers.Conv2D
        LSTM = tf.keras.layers.LSTM
        last_Dense = tf.keras.layers.Dense
    elif ensemble_type == 'mc_dropout':
        Dense = dense_dropout(kwargs['dropout_rate'])
        Conv2D = conv2d_dropout(kwargs['dropout_rate'])
        LSTM = lstm_dropout(kwargs['dropout_rate'])
        last_Dense = dropout_dense(kwargs['dropout_rate'])
    elif ensemble_type == 'bayesian':
        Dense = tfp.layers.DenseReparameterization
        Conv2D = tfp.layers.Convolution2DReparameterization
        LSTM = tf.keras.layers.LSTM
        last_Dense = tfp.layers.DenseReparameterization
    elif ensemble_type == 'anchor':
        Dense = dense_anchor(scale=kwargs['scale'], n_train=kwargs['batch_size'], seed=kwargs['seed'])
        Conv2D = conv2d_anchor(scale=kwargs['scale'], n_train=kwargs['batch_size'], seed=kwargs['seed'])
        LSTM = tf.keras.layers.LSTM
        last_Dense = dense_anchor(scale=kwargs['scale'], n_train=kwargs['batch_size'], seed=kwargs['seed'])
    else:
        raise ValueError('only support ensemble method {}.'.format(','.join(ensemble_method_scopes)))

    return Dense, Conv2D, LSTM, last_Dense


##### neural network initialization ###########

def get_fans(shape):
    fan_in = shape[0] if len(shape) == 2 else np.prod(shape[:-1])
    fan_out = shape[1] if len(shape) == 2 else shape[-1]
    return fan_in, fan_out


def glorot_uniform(shape):
    if len(shape) > 1:
        fan_in, fan_out = get_fans(shape)
        scale = np.sqrt(6. / (fan_in + fan_out))
        return np.random.uniform(low=-scale, high=scale, size=shape)
    else:
        return np.zeros(shape, dtype=np.float32)
