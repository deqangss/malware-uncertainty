from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf
import numpy as np
import sys

FLAGS = tf.compat.v1.flags.FLAGS

import os
import fileinput
import shutil
import warnings


class ParamWrapper(object):
    def __init__(self, params):
        if not isinstance(params, dict):
            params = vars(params)
        self.params = params

    def __getattr__(self, name):
        val = self.params.get(name)
        if val is None:
            MSG = "Setting params ({}) is deprecated"
            warnings.warn(MSG.format(name))
            val = FLAGS.__getattr__(name)
        return val


def retrive_files_set(base_dir, dir_ext, file_ext):
    """
    get file paths given the directory
    :param base_dir: basic directory
    :param dir_ext: directory append at the rear of base_dir
    :param file_ext: file extension
    :return: set of file paths. Avoid the repetition
    """

    def get_file_name(root_dir, file_ext):

        for dir_path, dir_names, file_names in os.walk(root_dir):
            for file_name in file_names:
                _ext = file_ext
                if os.path.splitext(file_name)[1] == _ext:
                    yield os.path.join(dir_path, file_name)
                elif '.' not in file_ext:
                    _ext = '.' + _ext

                    if os.path.splitext(file_name)[1] == _ext:
                        yield os.path.join(dir_path, file_name)
                    else:
                        pass
                else:
                    pass

    if file_ext is not None:
        file_exts = file_ext.split("|")
    else:
        file_exts = ['']
    file_path_set = set()
    for ext in file_exts:
        file_path_set = file_path_set | set(get_file_name(os.path.join(base_dir, dir_ext), ext))

    return file_path_set


def dump_pickle(data, path):
    try:
        import pickle as pkl
    except Exception as e:
        import cPickle as pkl

    if not os.path.exists(os.path.dirname(path)):
        mkdir(os.path.dirname(path))
    with open(path, 'wb') as wr:
        pkl.dump(data, wr)
    return True


def read_pickle(path):
    try:
        import pickle as pkl
    except Exception as e:
        import cPickle as pkl

    if os.path.isfile(path):
        with open(path, 'rb') as fr:
            return pkl.load(fr)
    else:
        raise IOError("The {0} is not been found.".format(path))


def dump_joblib(data, path):
    if not os.path.exists(os.path.dirname(path)):
        mkdir(os.path.dirname(path))

    try:
        from sklearn.externals import joblib
        with open(path, 'wb') as wr:
            joblib.dump(data, wr)
    except IOError:
        raise IOError("Dump data failed.")


def read_joblib(path):
    from sklearn.externals import joblib
    if os.path.isfile(path):
        with open(path, 'rb') as fr:
            return joblib.load(fr)
    else:
        raise IOError("The {0} is not a file.".format(path))


def read_txt(path, mode = 'r'):
    if os.path.isfile(path):
        with open(path, mode) as f_r:
            lines = f_r.read().strip().splitlines()
            return lines
    else:
        raise ValueError("{} does not seen like a file path.\n".format(path))


def dump_txt(data_str, path, mode = 'w'):
    if not isinstance(data_str, str):
        raise TypeError

    with open(path, mode) as f_w:
        f_w.write(data_str)

def readdata_np(data_path):
    try:
        with open(data_path, 'rb') as f_r:
            data = np.load(f_r)
        return data
    except IOError as e:
        raise IOError("Unable to open {0}: {1}.\n".format(data_path, str(e)))

def dumpdata_np(data, data_path):
    if not isinstance(data, np.ndarray):
        warnings.warn("The array is not the numpy.ndarray type.")
    data_dir = os.path.dirname(data_path)
    try:
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
        with open(data_path, 'wb') as f_s:
            np.save(f_s, data)
    except OSError as e:
        sys.stderr.write(e)


def load_json(json_path):
    try:
        import yaml
        with open(json_path, 'r') as rh:
            return yaml.safe_load(rh)
    except IOError as ex:
        raise IOError(str(ex) + ": Unable to load json file.")


def dump_json(obj_dict, file_path):
    try:
        import json
        with open(file_path, 'w+') as fh:
            json.dump(obj_dict, fh)
    except IOError as ex:
        raise IOError(str(ex) + ": Fail to dump dict using json toolbox")


def mkdir(target):
    try:
        if os.path.isfile(target):
            target = os.path.dirname(target)

        if not os.path.exists(target):
            os.makedirs(target)
        return 0
    except IOError as e:
        raise Exception("Fail to create directory! Error:" + str(e))


def copy_files(src_file_list, dst_dir):
    if not isinstance(src_file_list, list):
        raise TypeError
    if os.path.isdir(dst_dir):
        raise ValueError
    for src in src_file_list:
        if not os.path.isfile(src):
            continue
        shutil.copy(src, dst_dir)

# def generator(train_features, trainy, padding_opcodes):
#     N = len(train_features)
#     if trainy is not None:
#         for i in range(N):
#             data = train_features[i]
#             label = trainy[i]
#             data_padded = padding_opcodes(data)
#             yield data_padded, label
#     else:
#         for i in range(N):
#             data = train_features[i]
#             data_padded = padding_opcodes(data)
#             yield data_padded


def generator(train_features, trainy):
    def wrapper(func):
        def inner_f(*args):
            N = len(train_features)
            if trainy is not None:
                for i in range(N):
                    data = train_features[i]
                    label = trainy[i]
                    data_padded = func(data, *args[1:])
                    yield data_padded, label
            else:
                for i in range(N):
                    data = train_features[i]
                    data_padded = func(data, *args[1:])
                    yield data_padded
        return inner_f
    return wrapper


def train_template(detector_obj, train_tf_dataset, val_tf_dataset):
    """
    train procedure
    :param detector_obj: a detector class
    :param train_tf_dataset: Train data (x,y) in the format of tf.data.Datasest
    :param val_tf_dataset: Validation data (x,y) in the format of tf.data.Dataset
    :return: null
    """
    # training preparation: loss objective, optimizer, recording information objects
    loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)
    train_loss_recording = tf.keras.metrics.Mean(name='train_loss')
    train_acc_recording = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')
    optimizer = tf.keras.optimizers.Adam(detector_obj.hyper_parameters['learning_rate'])

    # model selection metrics
    val_loss_recording = tf.keras.metrics.Mean(name='val_loss')
    val_acc_recording = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')

    # one-step training
    @tf.function(experimental_relax_shapes=True)
    def train_step(x_tensor, y_tensor):
        with tf.GradientTape() as tape:
            _logits, _prob = self.model(x_tensor, training=True)
            _loss = loss_obj(y_tensor, _logits)
        gradients = tape.gradient(_loss, self.model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

        train_loss_recording(_loss)
        train_acc_recording(y_tensor, _prob)

    # one-step validation
    @tf.function(experimental_relax_shapes=True)
    def val_step(x_tensor, y_tensor):
        _logits, _prob = self.model(x_tensor, training=False)
        v_loss = loss_obj(y_tensor, _logits)
        val_loss_recording(v_loss)
        val_acc_recording(y_tensor, _prob)

    # start training
    best_val_acc = 0.
    for epoch in range(self.hyper_parameters['n_epochs']):
        train_loss_recording.reset_states()
        train_acc_recording.reset_states()
        val_loss_recording.reset_states()
        val_acc_recording.reset_states()

        total_time = 0.
        for _x, _y in train_ds:
            start_time = time.time()
            train_step(_x, _y)
            end_time = time.time()
            total_time += (end_time - start_time)

        for _x, _y in val_ds:
            start_time = time.time()
            val_step(_x, _y)
            end_time = time.time()
            total_time += (end_time - start_time)
        msg_train = 'Epoch {}, Train loss: {:.5}, Accuracy: {:.5}'
        print(msg_train.format(
            epoch + 1,
            train_loss_recording.result(),
            train_acc_recording.result() * 100))
        msg_val = '\t Validation loss: {:.5}, Accuracy: {:.5}.'
        print(msg_val.format(
            val_loss_recording.result(),
            val_acc_recording.result() * 100
        ))

        # model saving
        if best_val_acc < val_acc_recording.result():
            if not os.path.exists(os.path.dirname(self.save_dir)):
                utils.mkdir(os.path.dirname(self.save_dir))

            self.model.save_weights(self.save_dir)
            best_val_acc = val_acc_recording.result()
            msg_saving = '\t Saving Model at {} with best Accuracy {:.5}'
            print(msg_saving.format(self.save_dir, best_val_acc * 100))
