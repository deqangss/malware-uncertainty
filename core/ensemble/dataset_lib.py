""" This script is for building dataset """

import tensorflow as tf
from core.ensemble.model_hp import train_hparam


def build_dataset_from_numerical_data(data):
    """
    serialize the data to accommodate the format of model input
    :param data, tuple or np.ndarray
    """
    return tf.data.Dataset.from_tensor_slices(data). \
        cache(). \
        batch(train_hparam.batch_size)


def build_dataset_via_generator(generator, y=None):
    if y is not None:
        return tf.data.Dataset.from_generator(generator,
                                              output_types=(tf.int32, tf.int32),
                                              output_shapes=(tf.TensorShape([None]), tf.TensorShape([]))
                                              ). \
            cache(). \
            padded_batch(train_hparam.batch_size, padded_shapes=([None], []))
    else:
        return tf.data.Dataset.from_generator(generator,
                                              output_types=tf.int32,
                                              output_shapes=tf.TensorShape([None])
                                              ). \
            cache(). \
            padded_batch(train_hparam.batch_size, padded_shapes=([None]))


def build_dataset_from_img_generator(generator, input_dim, y=None):
    if y is not None:
        return tf.data.Dataset.from_generator(generator,
                                              output_types=(tf.float32, tf.float32),
                                              output_shapes=(tf.TensorShape([None, *input_dim]),
                                                             tf.TensorShape([None, ]))
                                              ).cache()
    else:
        return tf.data.Dataset.from_generator(generator,
                                              output_types=tf.float32,
                                              output_shapes=tf.TensorShape([None, *input_dim])
                                              ).cache()
