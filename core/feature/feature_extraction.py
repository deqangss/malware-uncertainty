import os
import multiprocessing
import collections
import warnings

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

from tools import utils
from tools import progressbar_wrapper
from core.ensemble.dataset_lib import \
    build_dataset_from_numerical_data, \
    build_dataset_via_generator, \
    build_dataset_from_img_generator


class FeatureExtraction(object):
    """Produce features for ML algorithms"""

    def __init__(self,
                 naive_data_save_dir,
                 meta_data_save_dir,
                 file_ext=None,
                 update=False,
                 proc_number=2):
        """
        initialization
        :param naive_data_save_dir: a directory for saving intermediates
        :param meta_data_save_dir: a directory for saving meta information
        :param file_ext: file extent
        :param update: boolean indicator for recomputing the naive features
        :param proc_number: process number
        """
        self.naive_data_save_dir = naive_data_save_dir
        self.meta_data_save_dir = meta_data_save_dir
        self.file_ext = file_ext
        self.update = update
        self.proc_number = int(proc_number)

    def feature_extraction(self, sample_dir, use_order_features=False):
        """
        extract the android features from Android packages and save the extractions into designed directory
        :param sample_dir: malicious / benign samples for the subsequent process of feature extraction
        :param use_order_features: following the order of the provided sample paths
        """
        raise NotImplementedError

    def feature_preprocess(self, features, gt_labels):
        """
        pre-processing the naive data to accommodate the input format of ML algorithms
        :param features: features produced by the method of feature_extraction
        :param gt_labels: corresponding ground truth labels
        """
        raise NotImplementedError

    def feature2ipt(self, features, labels=None):
        """
        Mapping features to the input space
        """
        raise NotImplementedError

    @staticmethod
    def _check(sample_dir):
        """
        check a valid directory and produce a list of file paths
        """
        sample_path_list = []

        if isinstance(sample_dir, str):
            if not os.path.exists(sample_dir):
                MSG = "No such directory or file {} exists!".format(sample_dir)
                raise ValueError(MSG)
            elif os.path.isfile(sample_dir):
                sample_path_list = [sample_dir]
            elif os.path.isdir(sample_dir):
                sample_path_list = list(utils.retrive_files_set(sample_dir, "", ".apk|"))
                assert len(sample_path_list) > 0, 'No files'
            else:
                raise ValueError(" No such path {}".format(sample_dir))
        elif isinstance(sample_dir, list):
            sample_path_list = [path for path in sample_dir if os.path.isfile(path)]
        else:
            MSG = "A directory or a list of paths are allowed!"
            raise ValueError(MSG)

        return sample_path_list


class DrebinFeature(FeatureExtraction):
    def __init__(self,
                 naive_data_save_dir,
                 meta_data_save_dir,
                 file_ext='.drebin',
                 update=False,
                 proc_number=2):
        super(DrebinFeature, self).__init__(naive_data_save_dir,
                                            meta_data_save_dir,
                                            file_ext,
                                            update,
                                            proc_number)

    def feature_extraction(self, sample_dir, use_order_features=False):
        """
        drebin features
        :return: 2D list, [[a list of features from an apk],...,[a list of features from an apk]]
        """
        from core.feature.drebin import AxplorerMapping, get_drebin_feature, load_feature

        sample_path_list = self._check(sample_dir)
        pool = multiprocessing.Pool(self.proc_number)
        pbar = progressbar_wrapper.ProgressBar()
        process_results = []
        tasks = []
        result_paths = []
        pmap = AxplorerMapping()

        for i, apk_path in enumerate(sample_path_list):
            sha256 = utils.get_sha256(apk_path)
            save_path = os.path.join(self.naive_data_save_dir, sha256 + self.file_ext)
            if os.path.exists(save_path) and (not self.update):
                result_paths.append(save_path)
                continue
            tasks.append(apk_path)
            process_results = pool.apply_async(get_drebin_feature,
                                               args=(apk_path, pmap, save_path),
                                               callback=pbar.CallbackForProgressBar)

        pool.close()
        if process_results:
            pbar.DisplayProgressBar(process_results, len(tasks), type='hour')
        pool.join()

        for i, res in enumerate(pbar.TotalResults):
            status, res_path = res
            if status:
                result_paths.append(res_path)

        feature_list = []
        if not use_order_features:
            for res_path in result_paths:
                feature_list.append(load_feature(res_path))
        else:
            for i, apk_path in enumerate(sample_path_list):
                sha256_code = utils.get_sha256(apk_path)
                save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)
                if os.path.exists(save_path):
                    feature_list.append(load_feature(save_path))
                else:
                    warnings.warn("Fail to perform feature extraction for '{}'".format(apk_path))

        return feature_list

    def feature_preprocess(self, features, gt_labels):
        """
        pre-processing the naive data to accommodate the input format of ML algorithms
        :param features: features produced by the method of feature_extraction
        :param gt_labels: corresponding ground truth labels
        """
        assert len(features) == len(gt_labels)
        tmp_vocab = self.get_vocabulary(features)
        # we select 10,000 features
        selected_features = self.feature_selection(features, gt_labels, tmp_vocab, dim=10000)
        vocab = self.get_vocabulary(selected_features)
        feature_vec = self.get_feature_representation(selected_features, vocab)

        # save
        vocab_path = os.path.join(self.meta_data_save_dir, 'drebin.vocab')
        if self.update or (not os.path.exists(vocab_path)):
            utils.dump_pickle(vocab, vocab_path)
        return feature_vec, gt_labels, vocab

    def feature2ipt(self, features, labels=None):
        """
        Mapping features to the input space
        :param features: the features produced by the method of feature_extraction
        :param labels: the ground truth labels correspond to features
        :rtype tf.data.Dataset object
        """
        # load
        vocab_path = os.path.join(self.meta_data_save_dir, 'drebin.vocab')
        if not os.path.exists(vocab_path):
            raise ValueError("A vocabulary is needed.")
        vocab = utils.read_pickle(vocab_path)
        dataX_np = self.get_feature_representation(features, vocab)
        if labels is not None:
            return build_dataset_from_numerical_data((dataX_np, labels))
        else:
            return build_dataset_from_numerical_data(dataX_np)

    def feature_selection(self, train_features, train_y, vocab, dim):
        """
        feature selection
        :param train_features: 2D feature
        :type train_features: numpy object
        :param train_y: ground truth labels
        :param vocab: a list of words (i.e., features)
        :param dim: the number of remained words
        :return: chose vocab
        """
        is_malware = (train_y == 1)
        mal_features = np.array(train_features)[is_malware]
        ben_features = np.array(train_features)[~is_malware]

        if (len(mal_features) <= 0) or (len(ben_features) <= 0):
            return vocab

        mal_representations = self.get_feature_representation(mal_features, vocab)
        mal_frequency = np.sum(mal_representations, axis=0) / float(len(mal_features))
        ben_representations = self.get_feature_representation(ben_features, vocab)
        ben_frequency = np.sum(ben_representations, axis=0) / float(len(ben_features))

        # eliminate the words showing zero occurrence in apk files
        is_null_feature = np.all(mal_representations == 0, axis=0) & np.all(ben_representations, axis=0)
        vocab_filtered = list(np.array(vocab)[~is_null_feature])

        if len(vocab_filtered) <= dim:
            return vocab_filtered
        else:
            feature_frq_diff = np.abs(mal_frequency[~is_null_feature] - ben_frequency[~is_null_feature])
            position_flag = np.argsort(feature_frq_diff)[::-1][:dim]

            vocab_selected = []
            for p in position_flag:
                vocab_selected.append(vocab_filtered[p])
            return vocab_selected

    @staticmethod
    def get_vocabulary(feature_list):
        """
        obtain the vocabulary based on the feature
        :param feature_list: 2D list of naive feature
        :return: feature vocabulary
        """
        c = collections.Counter()

        for features in feature_list:
            for feature in features:
                c[feature] = c[feature] + 1

        vocab, count = zip(*c.items())
        return list(vocab)

    @staticmethod
    def get_feature_representation(feature_list, vocab):
        """
        mapping feature to numerical representation
        :param feature_list: 2D feature list with shape [number of files, number of feature]
        :param vocab: a list of words
        :return: 2D representation
        :rtype numpy.ndarray
        """
        N = len(feature_list)
        M = len(vocab)

        assert N > 0 and M > 0

        representations = np.zeros((N, M), dtype=np.float32)
        dictionary = dict(zip(vocab, range(len(vocab))))
        for i, features in enumerate(feature_list):
            if len(features) > 0:
                filled_positions = [idx for idx in list(map(dictionary.get, features)) if idx is not None]
                if len(filled_positions) != 0:
                    representations[i, filled_positions] = 1.
                else:
                    warnings.warn("Produce zero feature vector.")

        return representations


class OpcodeSeq(FeatureExtraction):
    """
    get opcode sequences
    """

    def __init__(self,
                 naive_data_save_dir,
                 meta_data_save_dir=None,
                 file_ext='.opcode',
                 update=False,
                 proc_number=2):
        """
        initialization
        :param naive_data_save_dir: a directory for saving intermediates
        :param file_ext: file extent
        :param update: boolean indicator for recomputing the naive features
        :param proc_number: process number
        """
        super(OpcodeSeq, self).__init__(naive_data_save_dir,
                                        meta_data_save_dir,
                                        file_ext,
                                        update,
                                        proc_number)

    def feature_extraction(self, sample_dir, use_order_features=False):
        from core.feature.opcodeseq import feature_extr_wrapper, read_opcode

        sample_path_list = self._check(sample_dir)
        pool = multiprocessing.Pool(self.proc_number)
        pbar = progressbar_wrapper.ProgressBar()
        process_results = []
        tasks = []
        result_paths = []

        for i, apk_path in enumerate(sample_path_list):
            sha256_code = utils.get_sha256(apk_path)
            save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)

            if os.path.exists(save_path) and not self.update:
                result_paths.append(save_path)
                continue
            tasks.append(apk_path)
            process_results = pool.apply_async(feature_extr_wrapper,
                                               args=(apk_path, save_path),
                                               callback=pbar.CallbackForProgressBar)

        pool.close()
        if process_results:
            pbar.DisplayProgressBar(process_results, len(tasks), type='hour')
        pool.join()

        for i, res in enumerate(pbar.TotalResults):
            result_paths.append(res)

        feature_list = []
        if not use_order_features:
            for res_path in result_paths:
                features = read_opcode(res_path)
                feature_list.append(features)
        else:
            for i, apk_path in enumerate(sample_path_list):
                sha256_code = utils.get_sha256(apk_path)
                save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)
                if os.path.exists(save_path):
                    feature_list.append(read_opcode(save_path))
                else:
                    warnings.warn("Fail to perform feature extraction for '{}'".format(apk_path))

        return feature_list

    def feature_preprocess(self, features, gt_labels):
        """
        pre-processing the naive data to accommodate the input format of ML algorithms
        :param features: features produced by the method of feature_extraction
        :param gt_labels: corresponding ground truth labels
        """
        return features, gt_labels

    def feature2ipt(self, features, labels=None):
        """
        Mapping features to the input space
        """

        def padding_opcodes(features_of_an_apk, padding_char=0):
            padding_seq = []
            from core.ensemble.model_hp import text_cnn_hparam

            padding_chars = [padding_char] * text_cnn_hparam.kernel_size
            for i, seq in enumerate(features_of_an_apk):
                padding_seq.extend(seq)
                padding_seq.extend(padding_chars)
            return np.array(padding_seq[:text_cnn_hparam.max_sequence_length])

        def generator():
            N = len(features)
            if labels is not None:
                for i in range(N):
                    data = features[i]
                    label = labels[i]
                    data_padded = padding_opcodes(data)
                    yield data_padded, label
            else:
                for i in range(N):
                    data = features[i]
                    data_padded = padding_opcodes(data)
                    yield data_padded

        return build_dataset_via_generator(generator)


class MultiModality(FeatureExtraction):
    def __init__(self,
                 naive_data_save_dir,
                 meta_data_save_dir,
                 use_feature_selection=True,
                 feature_dimension=10000,
                 cluster_centers=100,
                 similar_threshold=0.5,
                 file_ext='.multimod',
                 update=False,
                 proc_number=2
                 ):
        """
        initialization
        :param naive_data_save_dir: a directory for saving intermediates
        :param meta_data_save_dir: a directory for saving meta information
        :param use_feature_selection: select features with top frequencies
        :param feature_dimension: the number of selected features, default 10,000
        :param cluster_centers: the number of cluster centers, default 100
        :param file_ext: file extent
        :param update: boolean indicator for recomputing the naive features
        :param proc_number: process number
        """
        super(MultiModality, self).__init__(naive_data_save_dir,
                                            meta_data_save_dir,
                                            file_ext,
                                            update,
                                            proc_number
                                            )
        self.use_feature_selection = use_feature_selection
        self.feature_dimension = feature_dimension
        self.cluster_centers = cluster_centers
        self.similar_threshold = similar_threshold

    def feature_extraction(self, sample_dir, use_order_features=False):
        """
        extract the android features from Android packages and save the extractions into designed directory
        """
        from core.feature.multimodality import API_LIST, get_multimod_feature, load_feature

        sample_path_list = self._check(sample_dir)
        pool = multiprocessing.Pool(self.proc_number)
        pbar = progressbar_wrapper.ProgressBar()
        process_results = []
        tasks = []
        result_paths = []

        for i, apk_path in enumerate(sample_path_list):
            sha256_code = utils.get_sha256(apk_path)
            save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)

            if os.path.exists(save_path) and (not self.update):
                result_paths.append(save_path)
                continue
            tasks.append(apk_path)
            process_results = pool.apply_async(get_multimod_feature,
                                               args=(apk_path, API_LIST, save_path),
                                               callback=pbar.CallbackForProgressBar)

        pool.close()
        if process_results:
            pbar.DisplayProgressBar(process_results, len(tasks), type='hour')
        pool.join()

        for i, res in enumerate(pbar.TotalResults):
            status, res_path = res
            if status:
                result_paths.append(res_path)

        feature_list = []
        if not use_order_features:
            for res_path in result_paths:
                features = load_feature(res_path)
                feature_list.append(features)
        else:
            for i, apk_path in enumerate(sample_path_list):
                sha256_code = utils.get_sha256(apk_path)
                save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)
                if os.path.exists(save_path):
                    feature_list.append(load_feature(save_path))
                else:
                    warnings.warn("Fail to perform feature extraction for '{}'".format(apk_path))

        return feature_list

    def feature_preprocess(self, features, gt_labels):
        """
        pre-processing the naive data to accommodate the input format of ML algorithms
        :param features: features produced by the method of feature_extraction, 2D list [[feature type 1,...,feature type 5],...,]
        :param gt_labels: corresponding ground truth labels
        """
        assert len(features) == len(gt_labels), 'inconsistent dataset'

        vocab_list = self.get_vocab(features,
                                    gt_labels,
                                    self.use_feature_selection,
                                    self.feature_dimension)
        # saving
        vocab_path = os.path.join(self.meta_data_save_dir, 'multimodality.vocab')
        self.save_meta_info(vocab_list, vocab_path)

        dataX_list = self.feature_mapping(features, vocab_list)

        # further processing
        scaled_dataX_list, scaler_list = self.data_scaling(dataX_list)

        # clustering for last three types of features
        cluster_centers = []
        for i, dataX in enumerate(scaled_dataX_list[2:]):  # produce the last three similarity-based feature
            center_vec = self.k_means_clustering(dataX,
                                                 self.cluster_centers)
            scaled_dataX_list[2 + i] = self._get_similarity(dataX, center_vec, self.similar_threshold)
            cluster_centers.append(center_vec)
        # saving
        scaler_path = os.path.join(self.meta_data_save_dir, 'multimodality.scaler')
        self.save_meta_info(scaler_list, scaler_path)
        cluster_center_path = os.path.join(self.meta_data_save_dir, 'multimodality.center')
        self.save_meta_info(cluster_centers, cluster_center_path)

        return scaled_dataX_list, gt_labels

    def feature2ipt(self, features, labels=None):
        """
        Mapping features to the input space
        """
        assert self._check_features(features)

        vocab_path = os.path.join(self.meta_data_save_dir, 'multimodality.vocab')
        vocab_list = self.load_meta_info(vocab_path)
        scaler_path = os.path.join(self.meta_data_save_dir, 'multimodality.scaler')
        scalers = self.load_meta_info(scaler_path)
        cluster_center_path = os.path.join(self.meta_data_save_dir, 'multimodality.center')
        centers = self.load_meta_info(cluster_center_path)

        dataX_list = self.feature_mapping(features, vocab_list)
        for i, dataX in enumerate(dataX_list):
            dataX_list[i] = scalers[i].transform(dataX)

        for i, center in enumerate(centers):
            dataX_list[2 + i] = self._get_similarity(dataX_list[2 + i], center, self.similar_threshold)

        # build dataset
        data_tf = build_dataset_from_numerical_data(tuple(dataX_list))
        if labels is not None:
            y = build_dataset_from_numerical_data(labels)
            from tensorflow import data
            return data.Dataset.zip((data_tf, y))
        else:
            return data_tf

    def save_meta_info(self, data, path):
        if self.update or (not os.path.exists(path)):
            utils.dump_joblib(data, path)

    @staticmethod
    def load_meta_info(path):
        if os.path.exists(path):
            return utils.read_joblib(path)
        else:
            raise ValueError("No such data.")

    @staticmethod
    def get_vocab(feature_list, gt_labels=None, use_feature_selection=False, dim=10000):
        """
        build vocabulary for five kinds of feature, including permission/component/environment, string, method api,
        method opcodes, shared library, each of which are presented in the 'collections.defaultdict' format
        :param feature_list: 2D shape, each row denotes features (dict format) extracted from an apk
        :param gt_labels: ground truth labels (optional)
        :param use_feature_selection: conducting feature extraction or not (False means no, and True means yes)
        :param dim: the number of selected feature (optional)
        :return: list of vocabularies corresponding to five kinds of feature
        """
        assert isinstance(feature_list, list) and len(feature_list) > 0, 'Type: {} and length: {}'.format(
            type(feature_list), len(feature_list))

        feature_np = np.array(feature_list, dtype=collections.defaultdict, ndmin=2)
        if len(feature_np.shape) != 2:
            warnings.warn("Feature extraction may be failed for some instances!")

        number_of_types = feature_np.shape[1]
        vocabulary_list = []
        for t in range(number_of_types):
            feature_dicts = feature_np[:, t]  # a column of a type of feature (dict container)
            c = collections.Counter()
            for fd in feature_dicts:
                for k, v in fd.items():
                    c[k] += v
            if not use_feature_selection:
                if len(c) > 0:
                    vocab, count = zip(*c.items())
                else:
                    vocab = []
            else:
                if len(c) > 0:
                    vocab, count = zip(*c.most_common(dim))  # filter out words with low frequency
                else:
                    vocab = []

            vocabulary_list.append(list(vocab))

        return vocabulary_list

    @staticmethod
    def feature_mapping(feature_list, vocab_list):
        """
        mapping feature to numerical representation
        :param feature_list: feature with shape [number of files, number of dictionary of features]
        :param vocab_list: several lists of words
        :return: 2D representation
        :rtype numpy.ndarray
        """
        assert len(feature_list[0]) == len(vocab_list)
        number_of_feature_types = len(vocab_list)

        representation_list = []
        for t in range(number_of_feature_types):
            feature_dict = np.array(feature_list)[:, t]
            N = len(feature_dict)
            vocab = vocab_list[t]
            M = len(list(vocab))
            representation = np.zeros((N, M), dtype=np.float32)
            dictionary = dict(zip(vocab, range(M)))
            for i, fd in enumerate(feature_dict):
                if len(fd) > 0:
                    filled_positions = [idx for idx in list(map(dictionary.get, list(fd.keys()))) if idx is not None]
                    filled_values = [fd.get(key) for key in list(fd.keys()) if dictionary.get(key) is not None]
                    if len(filled_positions) != 0:
                        representation[i, filled_positions] = filled_values[:]
                    else:
                        warnings.warn("Produce zero feature vector.")
            representation_list.append(representation)
        return representation_list

    @staticmethod
    def data_scaling(data_x_list):
        """
        minmax scaling for numerical feature representations
        :param data_x_list: a list of un-normalized feature representation
        :return: scaled feature representation
        :rtype : list of 2d numpy.ndarray
        """
        scalers = []
        for i, dataX in enumerate(data_x_list):
            scaler = MinMaxScaler()
            scaler.fit(dataX)
            data_x_list[i] = scaler.transform(dataX)
            scalers.append(scaler)
        return data_x_list, scalers

    @staticmethod
    def k_means_clustering(data_x, number_of_cluster_centers=100):
        N = data_x.shape[0]
        n_clusters = number_of_cluster_centers if number_of_cluster_centers < N else N // 2
        kmeans = KMeans(n_clusters=n_clusters,
                        random_state=0).fit(data_x)
        center = kmeans.cluster_centers_
        return center

    @staticmethod
    def _get_similarity(data_x, anchor, threshold=0.5):
        """
        get similarity matrix
        :return: similarity-based feature representation
        """
        similar_mat = np.min(1. / (np.square(data_x[:, np.newaxis, :] - anchor) + 1.),
                             axis=-1)  # note: RAM-consuming
        return np.greater(similar_mat, threshold).astype(np.float32)

    @staticmethod
    def _check_features(features):
        """
        check the completeness
        :param features: a list of features, each item presented in the 'collections.defaultdict' format
        :return: True or False
        """
        return (isinstance(features, list)) and (len(features) > 0) and (
            isinstance(features[0][0], dict))


class DexToImage(FeatureExtraction):
    """
    Convert the dex files to a RGB image
    """

    def __init__(self,
                 naive_data_save_dir,
                 meta_data_save_dir,
                 file_ext='.jpg',
                 update=False,
                 proc_number=2
                 ):
        super(DexToImage, self).__init__(naive_data_save_dir,
                                         meta_data_save_dir,
                                         file_ext,
                                         update,
                                         proc_number)

    def feature_extraction(self, sample_dir, use_order_features=False):
        from core.feature.dex2img import dex2img

        sample_path_list = self._check(sample_dir)
        pool = multiprocessing.Pool(self.proc_number)
        pbar = progressbar_wrapper.ProgressBar()
        process_results = []
        tasks = []
        result_paths = []

        for i, apk_path in enumerate(sample_path_list):
            sha256_code = utils.get_sha256(apk_path)
            save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)
            if os.path.exists(save_path) and (not self.update):
                result_paths.append(save_path)
                continue
            tasks.append(apk_path)
            process_results = pool.apply_async(dex2img,
                                               args=(apk_path, save_path),
                                               callback=pbar.CallbackForProgressBar)

        pool.close()
        if process_results:
            pbar.DisplayProgressBar(process_results, len(tasks), type='hour')
        pool.join()

        if not use_order_features:
            for i, res in enumerate(pbar.TotalResults):
                status, res_path = res
                if status:
                    result_paths.append(res_path)  # the order will be changed
        else:
            result_paths = []
            for i, apk_path in enumerate(sample_path_list):
                sha256_code = utils.get_sha256(apk_path)
                res_path = os.path.join(self.naive_data_save_dir, sha256_code + '.data')
                if os.path.exists(res_path):
                    result_paths.append(res_path)
                else:
                    warnings.warn("Fail to perform feature extraction for '{}'".format(apk_path))

        return result_paths

    def feature_preprocess(self, features, gt_labels):
        """
        pre-processing the naive data to accommodate the input format of ML algorithms
        :param features: a list of paths directing to image files
        :param gt_labels: corresponding ground truth labels
        """
        return features, gt_labels

    def feature2ipt(self, features, labels=None, image_size=[299, 299]):
        """
        Mapping features to the input space
        """
        image_names = [os.path.basename(feature) for feature in features]
        from tensorflow.keras.preprocessing.image import ImageDataGenerator
        img_generator_obj = ImageDataGenerator(rescale=1. / 255)
        from core.ensemble.model_hp import train_hparam
        if labels is not None:
            labels_string = [(lambda x: 'malware' if x else 'benware')(label) for label in labels]
            img_pd = pd.DataFrame({'image': image_names, 'labels': labels_string})
            generator = img_generator_obj.flow_from_dataframe(img_pd,
                                                              directory=self.naive_data_save_dir,
                                                              x_col='image',
                                                              y_col='labels',
                                                              classes=['benware', 'malware'],
                                                              target_size=image_size,
                                                              class_mode='binary',
                                                              shuffle=False,
                                                              batch_size=train_hparam.batch_size
                                                              )
        else:
            img_pd = pd.DataFrame({'image': image_names})
            generator = img_generator_obj.flow_from_dataframe(img_pd,
                                                              directory=self.naive_data_save_dir,
                                                              x_col='image',
                                                              target_size=image_size,
                                                              shuffle=False,
                                                              batch_size=train_hparam.batch_size
                                                              )

        return build_dataset_from_img_generator(generator)


class APISequence(FeatureExtraction):
    """Obtain api sequences based on the function call graph"""

    def __init__(self,
                 naive_data_save_dir,
                 meta_data_save_dir,
                 use_feature_selection=True,
                 ratio=0.25,
                 file_ext='.seq',
                 update=False,
                 proc_number=2
                 ):
        """
        initialization
        :param naive_data_save_dir: a directory for saving intermediates
        :param meta_data_save_dir: a directory for saving meta information
        :param use_feature_selection: use feature selection to filtering out entities with high frequencies
        :param ratio: resides the range of [0, 1] and denotes a portion of features will be neglected
        :param file_ext: file extent
        :param update: boolean indicator for recomputing the naive features
        :param proc_number: process number
        """
        super(APISequence, self).__init__(naive_data_save_dir,
                                          meta_data_save_dir,
                                          file_ext,
                                          update,
                                          proc_number)
        self.use_feature_selection = use_feature_selection
        self.ratio = ratio

    def feature_extraction(self, sample_dir, use_order_features=False):
        from core.feature.apiseq import get_api_sequence, load_feature

        sample_path_list = self._check(sample_dir)
        pool = multiprocessing.Pool(self.proc_number)
        pbar = progressbar_wrapper.ProgressBar()
        process_results = []
        tasks = []
        result_paths = []

        for i, apk_path in enumerate(sample_path_list):
            sha256_code = utils.get_sha256(apk_path)

            save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)
            if os.path.exists(save_path) and (not self.update):
                result_paths.append(save_path)
                continue
            tasks.append(apk_path)
            process_results = pool.apply_async(get_api_sequence,
                                               args=(apk_path, save_path),
                                               callback=pbar.CallbackForProgressBar)

        pool.close()
        if process_results:
            pbar.DisplayProgressBar(process_results, len(tasks), type='hour')
        pool.join()

        for i, res in enumerate(pbar.TotalResults):
            status, res_path = res
            if status:
                result_paths.append(res_path)

        feature_list = []
        if not use_order_features:
            for res_path in result_paths:
                features = load_feature(res_path)
                feature_list.append(features)
        else:
            for i, apk_path in enumerate(sample_path_list):
                sha256_code = utils.get_sha256(apk_path)
                save_path = os.path.join(self.naive_data_save_dir, sha256_code + self.file_ext)
                if os.path.exists(save_path):
                    feature_list.append(load_feature(save_path))
                else:
                    warnings.warn("Fail to perform feature extraction for '{}'".format(apk_path))

        return feature_list

    def feature_preprocess(self, features, gt_labels):
        """
        pre-processing the naive data to accommodate the input format of ML algorithms
        """
        vocab = self.get_vocab(features, gt_labels)

        dictionary = dict(zip(vocab, range(len(vocab))))

        discrete_feature = self.feature_mapping(features, dictionary)

        # saving
        dict_saving_path = os.path.join(self.meta_data_save_dir, 'apiseq.dict')
        if self.update or (not os.path.exists(dict_saving_path)):
            print(dictionary)
            utils.dump_joblib(dictionary, dict_saving_path)

        return discrete_feature, gt_labels

    def feature2ipt(self, features, labels=None):
        """
        Mapping features to the input space
        """
        dict_saving_path = os.path.join(self.meta_data_save_dir, 'apiseq.dict')
        if os.path.exists(dict_saving_path):
            dictionary = utils.read_joblib(dict_saving_path)
        else:
            raise ValueError("No meta data")

        discrete_features = self.feature_mapping(features, dictionary)

        def generator():
            if labels is not None:
                for data, label in zip(*(discrete_features, labels)):
                    yield data, label
            else:
                for data in discrete_features:
                    yield data

        return build_dataset_via_generator(generator)

    def get_vocab(self, feature_list, gt_labels):
        """
            create vocabulary based on a list of feature
            :param feature_list: 2D list with inconsistent column number, each row denotes feature extracted from an apk,
            :param gt_labels: ground truth labels
            :return: vocabulary
            :rtype: list
            """
        if self.use_feature_selection:
            assert 0. < self.ratio <= 1., 'the ratio should be (0,1]'

        c_mal = collections.Counter()
        c_ben = collections.Counter()

        for features, label in zip(feature_list, gt_labels):
            if label:
                c_mal.update(features)
            else:
                c_ben.update(features)

        if not self.use_feature_selection:
            c_mal.update(c_ben)
            c_all = c_mal
        else:
            api_num_mal = len(c_mal)
            api_hf_mal = c_mal.most_common(int(api_num_mal * self.ratio))
            api_num_ben = len(c_ben)
            api_hf_ben = c_ben.most_common(int(api_num_ben * self.ratio))
            common_apis = [e for e in api_hf_mal if e in api_hf_ben]
            for api in common_apis:
                c_mal[api] = 0
                c_ben[api] = 0
            c_mal.update(c_ben)
            c_all = c_mal

        vocab, count = zip(*c_all.items())
        return list(vocab)

    @staticmethod
    def feature_mapping(feature_list, dictionary):
        """
        mapping feature to numerical representation
        :param feature_list: feature with shape [number of files, number of dictionary of features]
        :param dictionary: vocabulary -> index
        :return: 2D representation
        :rtype numpy.ndarray
        """
        numerical_features = []
        for i, features in enumerate(feature_list):
            if len(features) > 0:
                numerical_features = [idx for idx in list(map(dictionary.get, features)) if idx is not None]
                if len(numerical_features) == 0:
                    warnings.warn("Produce zero feature vector.")
                numerical_features.append(numerical_features)
        return numerical_features
