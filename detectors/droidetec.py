"""
DROIDETEC: Android Malware Detection and Malicious Code Localization through Deep Learning

The implement is based on our understanding of the paper, entitled
``DROIDETEC: Android Malware Detection and Malicious Code Localization through Deep Learning'':
@article{ma2020droidetec,
  title={Droidetec: Android malware detection and malicious code localization through deep learning},
  author={Ma, Zhuo and Ge, Haoran and Wang, Zhuzhu and Liu, Yang and Liu, Ximeng},
  journal={arXiv preprint arXiv:2002.03594},
  year={2020}
}
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
from detectors.dnn import DNNDetector
from detectors.apiseq.feature_extraction import feature_extraction, get_vocab, get_discrete_representation

from tools import utils
from config import config, logging

logger = logging.getLogger("deep-android-malware-detection")

HYPER_PARAMs = {
    'random_seed': 23456,
    'use_feature_selection': True,
    'n_embedding_dim': 8,  # number of dims in lookupTable for projecting API to embedding codes
    'lstm_units': 64
    'hidden_units': [200],
    'dropout_rate': 0.5,
    'max_sequence_length': 1000000,
    'output_dim': 1,  # binary classification
    'n_epochs': 150,
    'batch_size': 2,  # todo: to be large
    'learning_rate': 0.001,
    'optimizer': 'adam'
}


class BiLSTMAttention(tf.keras.models.Model):
    def __init__(self, hyper_params=None):
        super(BiLSTMAttention, self).__init__()
        if hyper_params is not None:
            self.hyper_params = hyper_params
        else:
            self.hyper_params = HYPER_PARAMs

    def build(self, input_shape = 10000):
        self.embedding = tf.keras.layers.Embedding(input_shape,
                                                   self.hyper_params['n_embedding_dim'])

        self.bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.hyper_params['lstm_units'],
                                                                          return_sequences = True))
        self.dense_layer = tf.keras.layers.Dense(self.hyper_params['lstm_units'], use_bias=False)
        self.out_layer = tf.keras.layers.Dense(self.hyper_params['output_dim'])

    def call(self, x, training=False):
        embed_x = self.embedding(x)
        stateful_x = self.bi_lstm(embed_x)
        alpha_wights = tf.nn.softmax(self.dense_layer(tf.nn.tanh(stateful_x)), axis = 1)
        attn_x = tf.reduce_sum(alpha_wights * stateful_x, axis = 1)
        out = self.out_layer(attn_x)
        return out, tf.nn.sigmoid(out)


class Droidetec(DNNDetector):
    """Learn attention-based Bi-LSTM model from a variant of Function Call Graph (CFG)"""

    def __init__(self, model_obj=None,
                 hyper_parameters=None,
                 name='DROIDETEC'
                 ):
        super(Droidetec, self).__init__()
        self.model_obj = model_obj
        self.hyper_parameters = hyper_parameters
        if self.model_obj is None:
            self.model_obj = BiLSTMAttention
        if self.hyper_parameters is None:
            self.hyper_parameters = HYPER_PARAMs

        # todo: add data pre-processing

        self.model = self.model_obj(self.hyper_parameters)
        self.name = name
        self.save_dir = config.get('experiments', self.name.lower())

    def _data_preprocess(self, mal_sample_path=None, ben_sample_path=None):
        """
        data pre-processing
        :param mal_sample_path: a path links to the folder contains the malicious applications
        :param ben_sample_path: a path links to the folder contains the benign applications
        :return: tuple of 2D arraies, including trainX, valX, trainy, valy
        """
        dataX_path = config.get('feature.feature.api_sequence', 'dataX')
        datay_path = config.get('feature.feature.api_sequence', 'datay')
        if os.path.exists(datay_path) and os.path.exists(dataX_path):
            train_features, _1, val_features = utils.read_joblib(dataX_path)
            trainy, _1, valy = utils.read_joblib(datay_path)
            return train_features, val_features, trainy, valy
        if mal_sample_path is None and ben_sample_path is None:
            logger.error("No data. Exit!")
            sys.exit(1)

        mal_feature_list = feature_extraction(mal_sample_path, proc_number=2)
        ben_feature_list = feature_extraction(ben_sample_path, proc_number=2)
        feature_list = mal_feature_list + ben_feature_list
        gt_labels = np.zeros((len(mal_feature_list) + len(ben_feature_list)), dtype=np.int32)
        gt_labels[:len(mal_feature_list)] = 1

        vocab = get_vocab(feature_list, gt_labels, use_feature_selection=self.hyper_parameters['use_feature_selection'])

        dictionary = dict(zip(vocab, range(len(vocab))))

        numerical_feature, gt_labels = get_discrete_representation(feature_list, gt_labels, dictionary)

        trainX, testX, trainy, testy = \
            train_test_split(np.array(numerical_feature), gt_labels, test_size=0.2, random_state=0)
        trainX, valX, trainy, valy = \
            train_test_split(trainX, trainy, test_size=0.25, random_state=0)

        # saving
        utils.dump_json(dictionary, config.get('feature.api_sequence', 'dictionary'))
        utils.dump_joblib([trainX, testX, valX], config.get('feature.api_sequence', 'dataX'))
        utils.dump_joblib([trainy, testy, valy], config.get('feature.api_sequence', 'datay'))

        return trainX, valX, trainy, valy


    def _data_standardization(self, featureX, y=None, batch_size=32):
        """
        serialize the data to accommodate the data loader of tf.data.Dataset
        :param featureX: np.array([[list of variable length,list,list,...], ... ])
        :param y: ground truth label, if absent, will not be returned
        :return: an object belongs to tf.data.Dataset
        """

        @utils.generator(featureX, y)
        def gen_train_dataset(x):
            return x
        if y is not None:
            return tf.data.Dataset.from_generator(lambda: gen_train_dataset(featureX),
                                                  output_types=(tf.int32, tf.int32),
                                                  output_shapes=(tf.TensorShape([None]), tf.TensorShape([]))
                                                  ).cache().shuffle(buffer_size=1000).padded_batch(batch_size,
                                                                                                   padded_shapes=(
                                                                                                       [None], []))
        else:
            return tf.data.Dataset.from_generator(lambda: gen_train_dataset(featureX, y, lambda x: x),
                                                  output_types=tf.int32,
                                                  output_shapes=tf.TensorShape([None])
                                                  ).cache().shuffle(buffer_size=1000).padded_batch(batch_size,
                                                                                                   padded_shapes=(
                                                                                                       [None]))

    def get_feature_representation(self, apk_paths=None, labels=None, saving_path=None):
        # data preparation
        if saving_path is None:
            saving_path = config.get('feature.api_sequence', 'test')
        if os.path.exists(saving_path):
            testX, testy = utils.read_joblib(saving_path)
            return testX, testy

        feature_list = []
        gt_labels = []
        min_class_label, max_class_label = np.min(labels), np.max(labels)
        for labels in range(min_class_label, max_class_label + 1, 1):
            flag_label = (np.array(labels) == labels)
            apk_paths_label = np.array(apk_paths)[flag_label].to_list()
            features = feature_extraction(apk_paths_label, proc_number=1)
            feature_list.extend(features)
            gt_labels.extend([labels] * len(features))

        dictionary = utils.load_json(config.get('feature.api_sequence', 'dictionary'))

        test_features, gt_labels = get_discrete_representation(feature_list, gt_labels, dictionary)
        # saving
        utils.dump_joblib((test_features, gt_labels), saving_path)

        return test_features, gt_labels

    def load_test_data(self, apk_paths, gt_labels, feature_saving_path, use_handy_data):
        if use_handy_data:
            _, testX, _ = utils.read_joblib(config.get('feature.api_sequence', 'dataX'))
            _, gt_labels, _ = utils.read_joblib(config.get('feature.api_sequence', 'datay'))
        else:
            testX, gt_labels = self.get_feature_representation(apk_paths, gt_labels, feature_saving_path)
        return testX, gt_labels
