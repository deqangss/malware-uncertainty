"""
The deep neural network based malware detector.
The implement is based on the paper, entitled ``Adversarial Examples for Malware Detection'',
which can be found here:  http://patrickmcdaniel.org/pubs/esorics17.pdf

We slightly change the model architecture by reducing the number of neurons at the last layer to one.
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

import os
import sys
import time

sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
from detectors.detector import Detector
from detectors.drebin.feature_extraction import feature_extraction, \
    get_vocabulary, get_feature_representation, feature_selection
from tools import utils
from config import config, logging

logger = logging.getLogger("DNNDetector")

HYPER_PARAMs = {
    'random_seed': 23456,
    'feature_selection': True,
    'hidden_units': [200, 200],  # DNN has two hidden layers with each having 200 neurons
    'dropout_rate': 0.4,
    'output_dim': 1,  # binary classification
    'n_epochs': 150,
    'batch_size': 128,
    'learning_rate': 0.001,
    'optimizer': 'adam'
}


class DNN(tf.keras.models.Model):
    def __init__(self, hyper_params=None):
        super(DNN, self).__init__()
        if hyper_params is not None:
            self.hyper_params = hyper_params
        else:
            self.hyper_params = HYPER_PARAMs

    def build(self, input_shape=None):
        """Continue to initialize the model"""
        self.denses = [tf.keras.layers.Dense(neurons, activation='relu') for neurons in
                       self.hyper_params['hidden_units']]
        self.dropout = tf.keras.layers.Dropout(self.hyper_params['dropout_rate'])
        self.d_out = tf.keras.layers.Dense(self.hyper_params['output_dim'])

    def call(self, x, training=False):
        for i, h_layer in enumerate(self.denses):
            x = h_layer(x)
        x = self.dropout(x, training=training)
        logit = self.d_out(x)
        return logit, tf.nn.sigmoid(logit)

    def pred_prob(self, x, training=False):
        return tf.nn.sigmoid(self.call(x, training))

    def pred_label(self, x, threshold=0.5):
        return tf.cast(tf.math.greater_equal(self.call(x, False), threshold), tf.int64)


class DNNDetector(Detector):
    """Learn dnn model from drebin features"""

    def __init__(self, model_obj=None,
                 hyper_parameters=None,
                 name='DNNDETECTOR'
                 ):
        super(DNNDetector, self).__init__()
        self.model_obj = model_obj
        self.hyper_parameters = hyper_parameters
        if self.model_obj is None:
            self.model_obj = DNN
        if self.hyper_parameters is None:
            self.hyper_parameters = HYPER_PARAMs

        self.model = self.model_obj(self.hyper_parameters)
        self.feature_selection = self.hyper_parameters['feature_selection']
        self.name = name
        self.save_dir = config.get('experiments', self.name.lower())

    def get_feature_representation(self, apk_paths = None, labels = None, saving_path = None):
        """
        get feature representation (i.e., feature vector)
        :param apk_paths: a list of apk paths
        :param labels: corresponding labels
        :param saving_path: a path for saving feature array
        :return: features
        :rtype 2d numpy.ndarray [number_of_file, number_of_dimension]
        """
        # data preparation
        if saving_path is None:
            saving_path = config.get('feature.drebin', 'test')
        if os.path.exists(saving_path):
            testX, testy = utils.read_joblib(saving_path)
            return testX, testy

        apk_feature_list = []
        gt_labels = []
        min_class_label, max_class_label = np.min(labels), np.max(labels)
        for labels in range(min_class_label, max_class_label + 1, 1):
            flag_label = (np.array(labels) == labels)
            apk_paths_label = np.array(apk_paths)[flag_label].to_list()
            features = feature_extraction(apk_paths_label, backup_path=None, proc_number=1)
            apk_feature_list.extend(features)
            gt_labels.extend([labels] * len(features))

        vocab_path = config.get('feature.drebin', 'vocabulary')
        if not os.path.exists(vocab_path):
            logging.error("No vocabulary. Exist!")
            sys.exit(-1)
        vocab = utils.read_pickle(vocab_path)
        testX = get_feature_representation(apk_feature_list, vocab)
        # saving
        utils.dump_joblib((testX, labels), saving_path)
        return testX, gt_labels

    def _data_preprocess(self, mal_sample_path=None, ben_sample_path=None):
        """
        data pre-processing
        :param mal_sample_path: a path links to the folder contains the malicious applications
        :param ben_sample_path: a path links to the folder contains the benign applications
        :return: tuple of 2D arraies, including trainX, valX, trainy, valy
        """
        dataX_path = config.get('feature.drebin', 'dataX')
        datay_path = config.get('feature.drebin', 'datay')
        if os.path.exists(datay_path) and os.path.exists(dataX_path):
            trainX, _1, valX = utils.read_joblib(dataX_path)
            trainy, _1, valy = utils.read_joblib(datay_path)
            return trainX, valX, trainy, valy

        if (mal_sample_path is None) and (ben_sample_path is None):
            logger.error("No data. Exit!")
            sys.exit(1)

        mal_feature_list = feature_extraction(mal_sample_path, backup_path=None, proc_number=2)
        ben_feature_list = feature_extraction(ben_sample_path, backup_path=None, proc_number=2)
        feature_list = mal_feature_list + ben_feature_list
        gt_labels = np.zeros((len(mal_feature_list) + len(ben_feature_list)), dtype=np.int32)
        gt_labels[:len(mal_feature_list)] = 1

        vocab = get_vocabulary(feature_list)

        train_features, test_features, train_y, test_y = \
            train_test_split(feature_list, gt_labels, test_size=0.2, random_state=0)
        train_features, val_features, train_y, val_y = \
            train_test_split(train_features, train_y, test_size=0.25, random_state=0)

        if self.feature_selection:
            vocab = feature_selection(train_features, train_y, vocab, dim=10000)

        trainX = get_feature_representation(train_features, vocab)
        testX = get_feature_representation(test_features, vocab)
        valX = get_feature_representation(val_features, vocab)

        # saving
        utils.dump_pickle(vocab, config.get('feature.drebin', 'vocabulary'))
        utils.dump_joblib([trainX, testX, valX], config.get('feature.drebin', 'dataX'))
        utils.dump_joblib([train_y, test_y, val_y], config.get('feature.drebin', 'datay'))

        return trainX, valX, train_y, val_y

    def _data_standardization(self, featureX, y = None, batch_size = 32):
        """
        serialize the data to accommodate the data loader of tf.data.Dataset
        :param featureX: np.array([[features], ... ])
        :param y: ground truth label
        :return: an object belongs to tf.data.Dataset
        """
        if y is not None:
            return tf.data.Dataset.from_tensor_slices((featureX, y))\
                .cache()\
                .shuffle(buffer_size=1000)\
                .batch(batch_size)
        else:
            return tf.data.Dataset.from_tensor_slices(featureX) \
                .cache() \
                .shuffle(buffer_size=1000) \
                .batch(batch_size)

    def train(self, mal_sample_path=None, ben_sample_path=None):
        """
        learn a DNN-based malware detector from drebin features
        :param mal_sample_path: a folder contains malicious applications
        :param ben_sample_path: a folder contains benign applications
        :return: void
        """
        # data pre-processing
        trainX, valX, trainy, valy = \
            self._data_preprocess(mal_sample_path, ben_sample_path)

        bs_train = self.hyper_parameters['batch_size'] if self.hyper_parameters['batch_size'] <= len(trainX) else len(
            trainX)
        train_ds = self._data_standardization(trainX, trainy, bs_train)
        bs_val = bs_train if bs_train <= len(valX) else len(valX)
        val_ds = self._data_standardization(valX, valy, bs_val)

        # training preparation
        loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)
        train_loss_recording = tf.keras.metrics.Mean(name='train_loss')
        train_acc_recording = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')
        optimizer = tf.keras.optimizers.Adam(self.hyper_parameters['learning_rate'])

        # model selection metrics
        val_loss_recording = tf.keras.metrics.Mean(name='val_loss')
        val_acc_recording = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')

        # one-step training
        @tf.function(experimental_relax_shapes=True)
        def train_step(x_tensor, y_tensor):
            with tf.GradientTape() as tape:
                _logits, _prob = self.model(x_tensor, training=True)
                _loss = loss_obj(y_tensor, _logits)
            gradients = tape.gradient(_loss, self.model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

            train_loss_recording(_loss)
            train_acc_recording(y_tensor, _prob)

        # one-step validation
        @tf.function(experimental_relax_shapes=True)
        def val_step(x_tensor, y_tensor):
            _logits, _prob = self.model(x_tensor, training=False)
            v_loss = loss_obj(y_tensor, _logits)
            val_loss_recording(v_loss)
            val_acc_recording(y_tensor, _prob)

        # start training
        best_val_acc = 0.
        for epoch in range(self.hyper_parameters['n_epochs']):
            train_loss_recording.reset_states()
            train_acc_recording.reset_states()
            val_loss_recording.reset_states()
            val_acc_recording.reset_states()

            total_time = 0.
            for _x, _y in train_ds:
                start_time = time.time()
                train_step(_x, _y)
                end_time = time.time()
                total_time += (end_time - start_time)

            for _x, _y in val_ds:
                start_time = time.time()
                val_step(_x, _y)
                end_time = time.time()
                total_time += (end_time - start_time)
            msg_train = 'Epoch {}, Train loss: {:.5}, Accuracy: {:.5}'
            print(msg_train.format(
                epoch + 1,
                train_loss_recording.result(),
                train_acc_recording.result() * 100))
            msg_val = '\t Validation loss: {:.5}, Accuracy: {:.5}.'
            print(msg_val.format(
                val_loss_recording.result(),
                val_acc_recording.result() * 100
            ))

            # model saving
            if best_val_acc < val_acc_recording.result():
                if not os.path.exists(os.path.dirname(self.save_dir)):
                    utils.mkdir(os.path.dirname(self.save_dir))

                tf.saved_model.save(self.model, self.save_dir)
                best_val_acc = val_acc_recording.result()
                msg_saving = '\t Saving Model at {} with best Accuracy {:.5}'
                print(msg_saving.format(self.save_dir, best_val_acc * 100))

    def load_test_data(self, apk_paths, gt_labels, feature_saving_path, use_handy_data):
        if use_handy_data:
            _, testX, _ = utils.read_joblib(config.get('feature.drebin', 'dataX'))
            _, gt_labels, _ = utils.read_joblib(config.get('feature.drebin', 'datay'))
        else:
            testX, gt_labels = self.get_feature_representation(apk_paths, gt_labels, feature_saving_path)
        return testX, gt_labels

    def test(self, apk_paths=None, gt_labels=None, feature_saving_path=None, is_single_class = False, use_handy_data=True):
        """
        test model upon dataset
        :param apk_paths: test apks
        :param gt_labels: corresponding labels
        :param feature_saving_path: a path for saving feature representation of test apks
        :param is_single_class: test data only has a class
        :param use_handy_data: use the testX and testy produced during data preprocessing
        :return: void
        """
        # data preparation
        logger.info("data preparation for testing")
        testX, gt_labels = self.load_test_data(apk_paths, gt_labels, feature_saving_path, use_handy_data)

        bs_test = self.hyper_parameters['batch_size'] if self.hyper_parameters['batch_size'] >= len(testX) else len(
            testX)
        test_data = self._data_standardization(testX, batch_size=bs_test)

        new_model = tf.saved_model.load(self.save_dir)
        x_prob = new_model.predict(test_data)[1]
        x_pred = (x_prob >= 0.5).astype(np.int64).squeeze()

        # metrics
        from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score
        accuracy = accuracy_score(gt_labels, x_pred)
        b_accuracy = balanced_accuracy_score(gt_labels, x_pred)

        MSG = "The accuracy on the test dataset is {:.5f}%"
        print(MSG.format(accuracy * 100))
        logger.info(MSG.format(accuracy * 100))
        MSG = "The balanced accuracy on the test dataset is {:.5f}%"
        print(MSG.format(b_accuracy * 100))
        logger.info(MSG.format(b_accuracy * 100))
        if not is_single_class:
            tn, fp, fn, tp = confusion_matrix(gt_labels, x_pred).ravel()

            fpr = fp / float(tn + fp)
            fnr = fn / float(tp + fn)
            f1 = f1_score(gt_labels, x_pred, average='binary')

            print("Other evaluation metrics we may need:")
            MSG = "False Negative Rate (FNR) is {:.5f}%, False Positive Rate (FPR) is {:.5f}%, F1 score is {:.5f}%"
            print(MSG.format(fnr * 100, fpr * 100, f1 * 100))
            logger.info(MSG.format(fnr * 100, fpr * 100, f1 * 100))


def _main():
    detector = DNNDetector()
    data_path = config.get('dataset', 'dataset_root')
    malware_dir_name = config.get('dataset', 'malware_dir_name')
    benware_dir_name = config.get('dataset', 'benware_dir_name')
    mal_data_path = os.path.join(data_path, malware_dir_name)
    ben_data_path = os.path.join(data_path, benware_dir_name)
    # detector.train(mal_data_path, ben_data_path)
    detector.test()
    return 0


if __name__ == '__main__':
    sys.exit(_main())
