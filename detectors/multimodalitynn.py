"""
A Multimodal Deep Learning Method for Android Malware Detection Using Various Features

The implement is based on our understanding of the paper, entitled
``A Multimodal Deep Learning Method for Android Malware Detection Using Various Features'':
@ARTICLE{8443370,
  author={T. {Kim} and B. {Kang} and M. {Rho} and S. {Sezer} and E. G. {Im}},
  journal={IEEE Transactions on Information Forensics and Security},
  title={A Multimodal Deep Learning Method for Android Malware Detection Using Various Features},
  year={2019},
  volume={14},
  number={3},
  pages={773-788},}
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

import os
import sys
import time

sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
from detectors.dnn import DNNDetector
from detectors.multimodality.feature_extraction import feature_extraction, get_vocab, get_feature_representation

from tools import utils
from config import config, logging

logger = logging.getLogger("multimodality-nn")

HYPER_PARAMs = {
    'random_seed': 23456,
    'feature_selection': True,
    'n_clusters' : 1000,
    'threshold' : 0.5,
    'n_modality': 5,
    'hidden_units': [500, 500, 200, 200],  # DNN has two hidden layers with each having 200 neurons
    'dropout_rate': 0.4,
    'output_dim': 1,  # binary classification
    'n_epochs': 150,
    'batch_size': 128,
    'learning_rate': 0.001,
    'optimizer': 'adam'
}


class HeadLayer(tf.keras.layers.Layer):
    def __init__(self, neurons = [500, 500], dropout_rate = 0.2):
        super(HeadLayer, self).__init__()
        self.neurons = neurons
        self.dropout_rate = dropout_rate

        self.layers = [tf.keras.layers.Dense(neuron, activation='relu') for neuron in self.neurons]
        self.dropouts = [tf.keras.layers.Dropout(self.dropout_rate) for _ in range(len(self.neurons))]

    def call(self, x, is_training = False):
        for idx, layer in enumerate(self.layers):
            x = self.dropouts[idx](layer(x), training = is_training)
        return x


class MultiModNN(tf.keras.models.Model):
    def __init__(self, hyper_params=None):
        super(MultiModNN, self).__init__()
        if hyper_params is not None:
            self.hyper_params = hyper_params
        else:
            self.hyper_params = HYPER_PARAMs

        self.header_layers = []
        for i in range(self.hyper_params['n_modality']):
            self.header_layers.append(HeadLayer(self.hyper_params['hidden_units'][:2]))
        self.final_hidden_layers = [
            tf.keras.layers.Dense(neuron, activation = 'relu') for neuron in \
            self.hyper_params['hidden_units'][2:]
        ]
        self.dropout = tf.keras.layers.Dropout(self.hyper_params['dropout_rate'])
        self.out_layer = tf.keras.layers.Dense(1)

    def call(self, x_list, training=False):
        header_outputs = []
        for i, header in enumerate(self.header_layers):
            header_outputs.append(header(x_list[i], training))
        final_x = tf.concat(header_outputs, axis = 1)
        for final_layer in self.final_hidden_layers:
            final_x = final_layer(final_x)
        final_x = self.dropout(final_x, training)
        logits = self.out_layer(final_x)
        return logits, tf.nn.sigmoid(logits)


class MultiModDetector(DNNDetector):
    """
    Learning a deep neural networks from multiple types of android features
    """

    def __init__(self, model_obj=None,
                 hyper_parameters=None,
                 name='MULTIMODALITYNN'
                 ):
        super(MultiModDetector, self).__init__()
        self.model_obj = model_obj
        self.hyper_parameters = hyper_parameters
        if self.model_obj is None:
            self.model_obj = MultiModNN
        if self.hyper_parameters is None:
            self.hyper_parameters = HYPER_PARAMs

        self.model = self.model_obj(self.hyper_parameters)
        self.name = name
        self.save_dir = config.get('experiments', self.name.lower())

    def _data_preprocess(self, mal_sample_path=None, ben_sample_path=None):
        """
        data pre-processing
        :param mal_sample_path: a path links to the folder contains the malicious applications
        :param ben_sample_path: a path links to the folder contains the benign applications
        :return: tuple of 2D arraies, including trainX, valX, trainy, valy
        """
        dataX_path = config.get('feature.multimodality', 'dataX')
        datay_path = config.get('feature.multimodality', 'datay')
        if os.path.exists(datay_path) and os.path.exists(dataX_path):
            trainX_list, _1, valX_list = utils.read_joblib(dataX_path)
            trainy, _1, valy = utils.read_joblib(datay_path)
            return trainX_list, valX_list, trainy, valy
        if (mal_sample_path is None) and (ben_sample_path is None):
            logger.error("No data. Exit!")
            sys.exit(1)

        mal_feature_list = feature_extraction(mal_sample_path, proc_number=2)
        ben_feature_list = feature_extraction(ben_sample_path, proc_number=2)
        feature_list = mal_feature_list + ben_feature_list
        gt_labels = np.zeros((len(mal_feature_list) + len(ben_feature_list)), dtype=np.int32)
        gt_labels[:len(mal_feature_list)] = 1

        vocabs, dataX_list, gt_labels = get_vocab(feature_list,
                                                 self.hyper_parameters['feature_selection'],
                                                 gt_labels=gt_labels,
                                                 dim=10000) # in spite of this, RAM might be ran out

        N = dataX_list[0].shape[0]
        # scale
        scalers = []
        for i, dataX in enumerate(dataX_list):
            scaler = MinMaxScaler()
            scaler.fit(dataX)
            dataX_list[i] = scaler.transform(dataX)
            scalers.append(scaler)

        # algorithm Kmeans for regenerating representation
        # leveraging default hyper-parameters of sklearn, except for the number of clusters
        centers_of_kmeans = []
        for i, dataX in enumerate(dataX_list[2:]): # produce the last three similarity-based features
            n_clusters = self.hyper_parameters['n_clusters'] if self.hyper_parameters['n_clusters'] < N else N // 2
            kmeans = KMeans(n_clusters=n_clusters,
                            random_state= self.hyper_parameters['random_seed']).fit(dataX)
            center = kmeans.cluster_centers_
            dataX_list[2 + i] = self._get_similarity(dataX, center)
            centers_of_kmeans.append(center)

        train_idx, test_idx, trainy, testy = \
            train_test_split(range(N), gt_labels, test_size=0.2, random_state=0)
        train_idx, val_idx, trainy, valy = \
            train_test_split(train_idx, trainy, test_size=0.25, random_state=0)

        trainX_list, testX_list, valX_list = [], [], []
        for dataX in dataX_list:
            trainX_list.append(dataX[train_idx])
            testX_list.append(dataX[test_idx])
            valX_list.append(dataX[val_idx])

        # saving
        utils.dump_pickle(vocabs, config.get('feature.multimodality', 'vocabulary'))
        utils.dump_joblib([trainX_list, testX_list, valX_list], config.get('feature.multimodality', 'dataX'))
        utils.dump_joblib([trainy, testy, valy], config.get('feature.multimodality', 'datay'))
        utils.dump_joblib(scalers, config.get('feature.multimodality', 'scaler'))
        utils.dump_joblib(centers_of_kmeans, config.get('feature.multimodality', 'center'))
        return trainX_list, valX_list, trainy, valy

    def _get_similarity(self, dataX, anchor):
        """
        get similarity matrix
        :param dataX: 2D data
        :param anchor: centers of cluster
        :return: similarity-based feature representation
        """
        similary_mat = np.min(1. / (np.square(dataX[:, np.newaxis, :] - anchor) + 1.), axis=-1)  # RAM-consumin
        return np.greater(similary_mat, self.hyper_parameters['threshold']).astype(np.float32)

    def _data_standardization(self, featureX, y=None, batch_size=32):
        """
        serialize the data to accommodate the data loader of tf.data.Dataset
        :param featureX: np.array([[features], ... ])
        :param y: ground truth label
        :return: an object belongs to tf.data.Dataset
        """
        if y is not None:
            return tf.data.Dataset.from_tensor_slices((*featureX, y)) \
                .cache() \
                .shuffle(buffer_size=1000) \
                .batch(batch_size)
        else:
            return tf.data.Dataset.from_tensor_slices((featureX[0], *featureX[1:])) \
                .cache() \
                .batch(batch_size)

    def train(self, mal_sample_path=None, ben_sample_path=None):
        """
        learn a DNN-based malware detector from drebin features
        :param mal_sample_path: a folder contains malicious applications
        :param ben_sample_path: a folder contains benign applications
        :return: void
        """
        # data pre-processing
        trainX_list, valX_list, trainy, valy = \
            self._data_preprocess(mal_sample_path, ben_sample_path)

        Nt = trainX_list[0].shape[0]
        bs_train = self.hyper_parameters['batch_size'] if self.hyper_parameters['batch_size'] <= Nt else Nt
        train_ds = self._data_standardization(trainX_list, trainy, bs_train)
        Nv = valX_list[0].shape[0]
        bs_val = bs_train if bs_train <= Nv else Nv
        val_ds = self._data_standardization(valX_list, valy, bs_val)

        # training preparation
        loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)
        train_loss_recording = tf.keras.metrics.Mean(name='train_loss')
        train_acc_recording = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')
        optimizer = tf.keras.optimizers.Adam(self.hyper_parameters['learning_rate'])

        # model selection metrics
        val_loss_recording = tf.keras.metrics.Mean(name='val_loss')
        val_acc_recording = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')

        # one-step training
        @tf.function
        def train_step(x_tensor_list, y_tensor):
            with tf.GradientTape() as tape:
                _logits, _prob = self.model(x_tensor_list, training=True)
                _loss = loss_obj(y_tensor, _logits)
            gradients = tape.gradient(_loss, self.model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

            train_loss_recording(_loss)
            train_acc_recording(y_tensor, _prob)

        # one-step validation
        @tf.function
        def val_step(x_tensor_list, y_tensor):
            _logits, _prob = self.model(x_tensor_list, training=False)
            v_loss = loss_obj(y_tensor, _logits)
            val_loss_recording(v_loss)
            val_acc_recording(y_tensor, _prob)

        # start training
        best_val_acc = 0.
        for epoch in range(self.hyper_parameters['n_epochs']):
            train_loss_recording.reset_states()
            train_acc_recording.reset_states()
            val_loss_recording.reset_states()
            val_acc_recording.reset_states()

            total_time = 0.
            for *_x, _y in train_ds:
                start_time = time.time()
                train_step(_x, _y)
                end_time = time.time()
                total_time += (end_time - start_time)

            for *_x, _y in val_ds:
                start_time = time.time()
                val_step(_x, _y)
                end_time = time.time()
                total_time += (end_time - start_time)
            msg_train = 'Epoch {}, Train loss: {:.5}, Accuracy: {:.5}'
            print(msg_train.format(
                epoch + 1,
                train_loss_recording.result(),
                train_acc_recording.result() * 100))
            msg_val = '\t Validation loss: {:.5}, Accuracy: {:.5}.'
            print(msg_val.format(
                val_loss_recording.result(),
                val_acc_recording.result() * 100
            ))

            # model saving
            if best_val_acc < val_acc_recording.result():
                if not os.path.exists(os.path.dirname(self.save_dir)):
                    utils.mkdir(os.path.dirname(self.save_dir))

                tf.saved_model.save(self.model, self.save_dir)
                best_val_acc = val_acc_recording.result()
                msg_saving = '\t Saving Model at {} with best Accuracy {:.5}'
                print(msg_saving.format(self.save_dir, best_val_acc * 100))

    def get_feature_representation(self, apk_paths=None, labels=None, saving_path=None):
        """
        get feature representation (i.e., feature vector)
        :param apk_paths: a list of apk paths
        :param labels: corresponding labels
        :param saving_path: a path for saving feature array
        :return: features
        :rtype a list of 2d numpy.ndarray [a type of feature representation, ...]
        """
        # data preparation
        if saving_path is None:
            saving_path = config.get('feature.multimodality', 'test')
        if os.path.exists(saving_path):
            testX, testy = utils.read_joblib(saving_path)
            return testX, testy

        apk_feature_list = []
        gt_labels = []
        min_class_label, max_class_label = np.min(labels), np.max(labels)
        for labels in range(min_class_label, max_class_label + 1, 1):
            flag_label = (np.array(labels) == labels)
            apk_paths_labelled = np.array(apk_paths)[flag_label].to_list()
            features = feature_extraction(apk_paths_labelled, proc_number=1)
            apk_feature_list.extend(features)
            gt_labels.extend([labels] * len(features))
        apk_feature_np = np.array(apk_feature_list, dtype=object, ndim = 2)

        vocab_path = config.get('feature.multimodality', 'vocabulary')
        if not os.path.exists(vocab_path):
            logging.error("No vocabulary. Exist!")
            sys.exit(-1)
        vocabs = utils.read_pickle(vocab_path)
        testX_list = []
        number_of_feature_types = apk_feature_np.shape[1]
        for i in range(number_of_feature_types):
            testX_list.append(get_feature_representation(apk_feature_np[:,i], vocabs[i]))

        cluster_center_path = config.get('feature.multimodality', 'center')
        if not os.path.exists(cluster_center_path):
            logging.error("No clusters. Exist!")
            sys.exit(-1)
        centers = utils.read_pickle(cluster_center_path)
        for i, testX in enumerate(testX_list[2:]):
            testX_list[2 + i] = self._get_similarity(testX, centers[i])

        # saving
        utils.dump_joblib((testX_list, labels), saving_path)
        return testX_list, gt_labels

    def load_test_data(self, apk_paths, gt_labels, feature_saving_path, use_handy_data):
        if use_handy_data:
            _1, testX_list, _2 = utils.read_joblib(config.get('feature.multimodality', 'dataX'))
            _1, gt_labels, _2 = utils.read_joblib(config.get('feature.multimodality', 'datay'))
        else:
            testX_list, gt_labels = self.get_feature_representation(apk_paths, gt_labels, feature_saving_path)
        return testX_list, gt_labels

    def test(self, apk_paths=None, gt_labels=None, feature_saving_path=None, is_single_class = False, use_handy_data=True):
        """
        test model upon dataset
        :param apk_paths: test apks
        :param gt_labels: corresponding labels
        :param feature_saving_path: a path for saving feature representation of test apks
        :param is_single_class: test data only has a class
        :param use_handy_data: use the testX and testy produced during data preprocessing
        :return: void
        """
        # data preparation
        logger.info("data preparation for testing")
        testX_list, gt_labels = self.load_test_data(apk_paths, gt_labels, feature_saving_path, use_handy_data)

        Nt = testX_list[0].shape[0]
        bs_test = self.hyper_parameters['batch_size'] if self.hyper_parameters['batch_size'] >= Nt else Nt
        test_data = self._data_standardization(testX_list, batch_size=bs_test)

        # new_model = self.model_obj(self.hyper_parameters)
        # load_status = new_model.load_weights(self.save_dir)
        # load_status.assert_consumed()
        new_model = tf.saved_model.load(self.save_dir)

        # one-step testing
        @tf.function
        def test_step(x_tensor_list):
            _1, _prob = new_model(x_tensor_list, training=False)
            return _prob

        x_prob_list = []
        for _x in test_data:
            x_prob_list.append(test_step(list(_x)))
        x_prob = np.concatenate(x_prob_list)
        x_pred = (x_prob >= 0.5).astype(np.int64).squeeze()

        # metrics
        from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score
        accuracy = accuracy_score(gt_labels, x_pred)
        b_accuracy = balanced_accuracy_score(gt_labels, x_pred)

        MSG = "The accuracy on the test dataset is {:.5f}%"
        print(MSG.format(accuracy * 100))
        logger.info(MSG.format(accuracy * 100))
        MSG = "The balanced accuracy on the test dataset is {:.5f}%"
        print(MSG.format(b_accuracy * 100))
        logger.info(MSG.format(b_accuracy * 100))
        if not is_single_class:
            tn, fp, fn, tp = confusion_matrix(gt_labels, x_pred).ravel()

            fpr = fp / float(tn + fp)
            fnr = fn / float(tp + fn)
            f1 = f1_score(gt_labels, x_pred, average='binary')

            print("Other evaluation metrics we may need:")
            MSG = "False Negative Rate (FNR) is {:.5f}%, False Positive Rate (FPR) is {:.5f}%, F1 score is {:.5f}%"
            print(MSG.format(fnr * 100, fpr * 100, f1 * 100))
            logger.info(MSG.format(fnr * 100, fpr * 100, f1 * 100))


def _main():
    detector = MultiModDetector()
    data_path = config.get('dataset', 'dataset_root')
    malware_dir_name = config.get('dataset', 'malware_dir_name')
    benware_dir_name = config.get('dataset', 'benware_dir_name')
    mal_data_path = os.path.join(data_path, malware_dir_name)
    ben_data_path = os.path.join(data_path, benware_dir_name)
    # detector._data_preprocess(mal_data_path, ben_data_path)
    detector.train(mal_data_path, ben_data_path)
    detector.test()
    return 0


if __name__ == '__main__':
    sys.exit(_main())
