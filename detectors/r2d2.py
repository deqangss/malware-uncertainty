"""
R2-D2: ColoR-inspired Convolutional NeuRal Network (CNN)-based AndroiD Malware Detections

The implement is based on our understanding of the paper, entitled
``R2-D2: ColoR-inspired Convolutional NeuRal Network (CNN)-based AndroiD Malware Detections'':
@INPROCEEDINGS{8622324,
  author={T. H. {Huang} and H. {Kao}},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)},
  title={R2-D2: ColoR-inspired Convolutional NeuRal Network (CNN)-based AndroiD Malware Detections},
  year={2018},
  volume={},
  number={},
  pages={2633-2642},}
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

import os
import sys
import time

sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
from detectors.dnn import DNNDetector
from detectors.dex2img.feature_extraction import feature_extraction
from tools import utils
from config import config, logging

logger = logging.getLogger("r2d2-nn")

HYPER_PARAMs = {
    'random_seed': 23456,
    'use_small_model': True,  # MobileNetV2 or InceptionV3
    'output_dim': 1,  # binary classification
    'n_epochs': 150,
    'batch_size': 2,
    'learning_rate': 0.0001,
    'optimizer': 'adam'
}


class InceptionCNN(tf.keras.models.Model):
    def __init__(self, hyper_params=None):
        super(InceptionCNN, self).__init__()
        if hyper_params is not None:
            self.hyper_params = hyper_params
        else:
            self.hyper_params = HYPER_PARAMs

        if self.hyper_params['use_small_model']:
            self.base_model = tf.keras.applications.MobileNetV2(include_top=False,
                                                                weights='imagenet'
                                                                )
        else:
            self.base_model = tf.keras.applications.MobileNetV2(include_top=False,
                                                                weights='imagenet'
                                                                )
        self.base_model.trainable = False

        self.pooling = tf.keras.layers.GlobalAveragePooling2D()
        self.out_layer = tf.keras.layers.Dense(self.hyper_params['output_dim'])

    def call(self, x):
        x = self.pooling(self.base_model(x))
        logits = self.out_layer(x)
        return logits, tf.nn.sigmoid(logits)


class R2D2(DNNDetector):
    """
    Learning a color-inspired convolutional neural networks (CNN)-based Android malware detection (R2-D2) system
    """

    def __init__(self,
                 model_obj=None,
                 hyper_parameters=None,
                 name='R2D2'
                 ):
        super(R2D2, self).__init__()
        self.model_obj = model_obj
        self.hyper_parameters = hyper_parameters
        if self.model_obj is None:
            self.model_obj = InceptionCNN
        if self.hyper_parameters is None:
            self.hyper_parameters = HYPER_PARAMs

        self.model = self.model_obj(self.hyper_parameters)
        self.name = name
        self.save_dir = config.get('experiments', self.name.lower())

    def _data_preprocess(self, mal_sample_path=None, ben_sample_path=None):
        """
        data pre-processing
        :param mal_sample_path: a path links to the folder contains the malicious applications
        :param ben_sample_path: a path links to the folder contains the benign applications
        :return: tuple of 2D arraies, including trainX, valX, trainy, valy
        """
        mal_path_list = feature_extraction(mal_sample_path, backup_path=None, proc_number=2)
        ben_path_list = feature_extraction(ben_sample_path, backup_path=None, proc_number=2)
        feature_list = mal_path_list + ben_path_list
        gt_labels = np.zeros((len(mal_path_list) + len(ben_path_list)), dtype=np.int32)
        gt_labels[:len(mal_path_list)] = 1

        train_data_paths, test_data_paths, trainy, testy = \
            train_test_split(feature_list, gt_labels, test_size=0.2, random_state=0)
        train_data_paths, val_data_paths, trainy, valy = \
            train_test_split(train_data_paths, trainy, test_size=0.25, random_state=0)

        # distribute images
        tr_dir = config.get('feature.image', 'train_dir')
        self.distribute_image(train_data_paths, tr_dir, trainy)
        test_dir = config.get('feature.image', 'test_dir')
        self.distribute_image(test_data_paths, test_dir, testy)
        val_dir = config.get('feature.image', 'val_dir')
        self.distribute_image(val_data_paths, val_dir, valy)
        return tr_dir, val_dir

    def distribute_image(self, data_paths, dst_dir, trainy):
        """
        distribute image paths to accommodate ImageDataGenerator
        :param data_paths: a list of paths
        :param dst_dir: destination directory
        :param trainy: label
        :return: void
        """
        import shutil

        mal_dir = os.path.join(dst_dir, 'mal')
        if os.path.exists(mal_dir):
            shutil.rmtree(mal_dir)
        utils.mkdir(mal_dir)
        ben_dir = os.path.join(dst_dir, 'ben')
        if os.path.exists(ben_dir):
            shutil.rmtree(ben_dir)
        utils.mkdir(ben_dir)

        for path, label in zip(data_paths, trainy):
            if label == 1:
                shutil.copy(path, mal_dir)
            elif label == 0:
                shutil.copy(path, ben_dir)
            else:
                raise ValueError
        return

    def _data_standardization(self, data_dir, input_shape=(299, 299), shuffle = True, batch_size=32):
        """
        serialize the data to accommodate the data loader of tf.data.Dataset
        :param data_dir: data directory
        :return: an object belongs to tf.data.Dataset
        """
        img_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)

        if shuffle:
            return img_generator.flow_from_directory(batch_size=batch_size,
                                                     directory=data_dir,
                                                     shuffle=True,
                                                     target_size=input_shape,
                                                     class_mode='binary'
                                                     )
        else:
            return img_generator.flow_from_directory(batch_size=batch_size,
                                                     directory=data_dir,
                                                     shuffle=False,
                                                     target_size=input_shape,
                                                     class_mode='binary'
                                                     )

    def get_feature_representation(self, apk_paths=None, labels=None, saving_path=None):
        """
        get feature representation (i.e., feature vector)
        :param apk_paths: a list of apk paths
        :param labels: corresponding labels
        :param saving_path: a path for saving feature array
        :return: features
        :rtype a list of 2d numpy.ndarray [a type of feature representation, ...]
        """
        # data preparation
        saving_path = config.get('feature.image', 'external_test_dir')

        mal_saving_path = os.path.join(saving_path, 'mal')
        mal_indicator = (labels == 1)
        apk_paths_labelled = np.array(apk_paths)[mal_indicator].to_list()
        feature_extraction(apk_paths_labelled, backup_path=mal_saving_path, proc_number=1)

        ben_saving_path = os.path.join(saving_path, 'ben')
        ben_indicator = (labels == 0)
        apk_paths_labelled = np.array(apk_paths)[ben_indicator].to_list()
        feature_extraction(apk_paths_labelled, backup_path=ben_saving_path, proc_number=1)
        return saving_path

    def load_test_data(self, apk_paths, gt_labels, feature_saving_path, use_handy_data):
        if use_handy_data:
            test_dir = config.get('feature.image', 'test_dir')
        else:
            test_dir = self.get_feature_representation(apk_paths, gt_labels, feature_saving_path)
        return test_dir

    def train(self, mal_sample_path=None, ben_sample_path=None):
        """
        learn a DNN-based malware detector from drebin features
        :param mal_sample_path: a folder contains malicious applications
        :param ben_sample_path: a folder contains benign applications
        :return: void
        """
        # data pre-processing
        train_dir, val_dir = \
            self._data_preprocess(mal_sample_path, ben_sample_path)

        train_ds = self._data_standardization(train_dir, batch_size=self.hyper_parameters['batch_size'])
        val_ds = self._data_standardization(val_dir, shuffle=False, batch_size=self.hyper_parameters['batch_size'])

        # training preparation
        loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)
        train_loss_recording = tf.keras.metrics.Mean(name='train_loss')
        train_acc_recording = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')
        optimizer = tf.keras.optimizers.Adam(self.hyper_parameters['learning_rate'])

        # model selection metrics
        val_loss_recording = tf.keras.metrics.Mean(name='val_loss')
        val_acc_recording = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')

        # one-step training
        @tf.function
        def train_step(x_tensor, y_tensor):
            with tf.GradientTape() as tape:
                _logits, _prob = self.model(x_tensor)
                _loss = loss_obj(y_tensor, _logits)
            gradients = tape.gradient(_loss, self.model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

            train_loss_recording(_loss)
            train_acc_recording(y_tensor, _prob)

        # one-step validation
        @tf.function
        def val_step(x_tensor, y_tensor):
            _logits, _prob = self.model(x_tensor)
            v_loss = loss_obj(y_tensor, _logits)
            val_loss_recording(v_loss)
            val_acc_recording(y_tensor, _prob)

        # start training
        best_val_acc = 0.
        for epoch in range(self.hyper_parameters['n_epochs']):
            train_loss_recording.reset_states()
            train_acc_recording.reset_states()
            val_loss_recording.reset_states()
            val_acc_recording.reset_states()

            total_time = 0.
            mini_batchs = 0
            for _x, _y in train_ds:
                start_time = time.time()
                train_step(_x, tf.cast(_y, tf.int32))
                end_time = time.time()
                total_time += (end_time - start_time)
                mini_batchs += 1
                if mini_batchs >= len(train_ds) * self.hyper_parameters['batch_size']:
                    break

            mini_batchs = 0
            for _x, _y in val_ds:
                start_time = time.time()
                val_step(_x, tf.cast(_y, tf.int32))
                end_time = time.time()
                total_time += (end_time - start_time)
                mini_batchs += 1
                if mini_batchs >= len(train_ds) * self.hyper_parameters['batch_size']:
                    break

            msg_train = 'Epoch {}, Train loss: {:.5}, Accuracy: {:.5}'
            print(msg_train.format(
                epoch + 1,
                train_loss_recording.result(),
                train_acc_recording.result() * 100))
            msg_val = '\t Validation loss: {:.5}, Accuracy: {:.5}.'
            print(msg_val.format(
                val_loss_recording.result(),
                val_acc_recording.result() * 100
            ))

            # model saving
            if best_val_acc < val_acc_recording.result():
                if not os.path.exists(os.path.dirname(self.save_dir)):
                    utils.mkdir(os.path.dirname(self.save_dir))

                tf.saved_model.save(self.model, self.save_dir)
                best_val_acc = val_acc_recording.result()
                msg_saving = '\t Saving Model at {} with best Accuracy {:.5}'
                print(msg_saving.format(self.save_dir, best_val_acc * 100))

    def test(self, apk_paths=None, gt_labels=None, feature_saving_path=None, is_single_class=False,
             use_handy_data=True):
        """
        test model upon dataset
        :param apk_paths: test apks
        :param gt_labels: corresponding labels
        :param feature_saving_path: a path for saving feature representation of test apks
        :param is_single_class: test data only has a class
        :param use_handy_data: use the testX and testy produced during data preprocessing
        :return: void
        """
        # data preparation
        logger.info("data preparation for testing")
        test_dir = self.load_test_data(apk_paths, gt_labels, feature_saving_path, use_handy_data)
        test_data = self._data_standardization(test_dir,
                                               batch_size= self.hyper_parameters['batch_size'],
                                               shuffle = False)

        new_model = tf.saved_model.load(self.save_dir)
        # one-step testing
        @tf.function
        def test_step(x):
            _1, _prob = new_model(x)
            return _prob

        x_prob_list = []
        y_true_list = []
        batches = 0
        Nt = 0
        for _x, y in test_data:
            y_true_list.append(y.astype(np.int32))
            x_prob_list.append(test_step(_x))
            batches += 1
            Nt += _x.shape[0]
            if batches >= len(test_data) * self.hyper_parameters['batch_size']:
                break

        gt_labels = np.concatenate(y_true_list)[:Nt]
        x_prob = np.concatenate(x_prob_list)[:Nt]
        x_pred = (x_prob >= 0.5).astype(np.int64).squeeze()

        # metrics
        from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, balanced_accuracy_score
        accuracy = accuracy_score(gt_labels, x_pred)
        b_accuracy = balanced_accuracy_score(gt_labels, x_pred)

        MSG = "The accuracy on the test dataset is {:.5f}%"
        print(MSG.format(accuracy * 100))
        logger.info(MSG.format(accuracy * 100))
        MSG = "The balanced accuracy on the test dataset is {:.5f}%"
        print(MSG.format(b_accuracy * 100))
        logger.info(MSG.format(b_accuracy * 100))
        if not is_single_class:
            tn, fp, fn, tp = confusion_matrix(gt_labels, x_pred).ravel()

            fpr = fp / float(tn + fp)
            fnr = fn / float(tp + fn)
            f1 = f1_score(gt_labels, x_pred, average='binary')

            print("Other evaluation metrics we may need:")
            MSG = "False Negative Rate (FNR) is {:.5f}%, False Positive Rate (FPR) is {:.5f}%, F1 score is {:.5f}%"
            print(MSG.format(fnr * 100, fpr * 100, f1 * 100))
            logger.info(MSG.format(fnr * 100, fpr * 100, f1 * 100))


def _main():
    detector = R2D2()
    data_path = config.get('dataset', 'dataset_root')
    malware_dir_name = config.get('dataset', 'malware_dir_name')
    benware_dir_name = config.get('dataset', 'benware_dir_name')
    mal_data_path = os.path.join(data_path, malware_dir_name)
    ben_data_path = os.path.join(data_path, benware_dir_name)
    # detector._data_preprocess(mal_data_path, ben_data_path)
    # detector.train(mal_data_path, ben_data_path)
    detector.test()
    return 0


if __name__ == '__main__':
    sys.exit(_main())
