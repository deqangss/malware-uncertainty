"""
deep android malware detection
The implement is based on the paper, entitled ``Deep Android Malware Detection'',
which can be found here:  https://dl.acm.org/doi/10.1145/3029806.3029823
"""

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))
from detectors.dnn import DNNDetector
from detectors.deepdroid.feature_extraction import feature_extraction

from tools import utils
from config import config, logging

logger = logging.getLogger("deep-android-malware-detection")

HYPER_PARAMs = {
    'random_seed': 23456,
    'vocab_size': 256,
    'n_embedding_dim': 8,  # number of dims in lookupTable for projecting instructions to network
    'n_conv_filters': 64,  # number of convolutional filters
    'n_kernel_length': 8,
    'hidden_units': [200, 200],
    'dropout_rate': 0.5,
    'use_spatial_dropout': False,
    'use_conv_dropout': False,
    'max_sequence_length': 10, # todo: to be large
    'output_dim': 1,  # binary classification
    'n_epochs': 150,
    'batch_size': 2,  # todo: to be large
    'learning_rate': 0.001,
    'optimizer': 'adam'
}


class TextCNN(tf.keras.models.Model):
    def __init__(self, hyper_params=None):
        super(TextCNN, self).__init__()
        if hyper_params is not None:
            self.hyper_params = hyper_params
        else:
            self.hyper_params = HYPER_PARAMs

        self.embedding = tf.keras.layers.Embedding(self.hyper_params['vocab_size'],
                                                   self.hyper_params['n_embedding_dim'])
        self.spatial_dropout = tf.keras.layers.SpatialDropout2D(rate=self.hyper_params['dropout_rate'])

        self.conv = tf.keras.layers.Conv2D(self.hyper_params['n_conv_filters'],
                                           self.hyper_params['n_kernel_length'],
                                           activation='relu'
                                           )
        self.conv_dropout = tf.keras.layers.Dropout(self.hyper_params['dropout_rate'])

        self.pooling = tf.keras.layers.GlobalMaxPool2D()  # produce a fixed length vector
        self.dense_layers = self.denses = [tf.keras.layers.Dense(neurons, activation='relu') for neurons in
                                           self.hyper_params['hidden_units']]
        self.dropout = tf.keras.layers.Dropout(self.hyper_params['dropout_rate'])
        self.d_out = tf.keras.layers.Dense(self.hyper_params['output_dim'])
        self.flatten = tf.keras.layers.Flatten()

    def call(self, x, training=False):
        embed_code = self.embedding(x)
        embed_code = embed_code[:, :, :, tf.newaxis]  # batch_size, seq_length, embedding_dim, 1
        if self.hyper_params['use_spatial_dropout']:
            embed_code = self.spatial_dropout(embed_code, training=training)

        conv_x = self.conv(embed_code)
        if self.hyper_params['use_conv_dropout']:
            conv_x = self.conv_dropout(conv_x)
        flatten_x = self.pooling(conv_x)

        for i, h_layer in enumerate(self.dense_layers):
            flatten_x = h_layer(flatten_x)
        flatten_x = self.dropout(flatten_x, training=training)
        logits = self.d_out(flatten_x)
        return logits, tf.nn.sigmoid(logits)


class DeepDroid(DNNDetector):
    """Learn text CNN model from OpCode sequences"""

    def __init__(self, model_obj=None,
                 hyper_parameters=None,
                 name='DEEPDROID'
                 ):
        super(DeepDroid, self).__init__()
        self.model_obj = model_obj
        self.hyper_parameters = hyper_parameters
        if self.model_obj is None:
            self.model_obj = TextCNN
        if self.hyper_parameters is None:
            self.hyper_parameters = HYPER_PARAMs

        self.model = self.model_obj(self.hyper_parameters)
        self.name = name
        self.save_dir = config.get('experiments', self.name.lower())

    def _data_preprocess(self, mal_sample_path=None, ben_sample_path=None):
        """
        data pre-processing
        :param mal_sample_path: a path links to the folder contains the malicious applications
        :param ben_sample_path: a path links to the folder contains the benign applications
        :return: tuple of 2D arraies, including trainX, valX, trainy, valy
        """
        dataX_path = config.get('feature.opcode', 'dataX')
        datay_path = config.get('feature.opcode', 'datay')
        if os.path.exists(datay_path) and os.path.exists(dataX_path):
            train_features, _1, val_features = utils.read_joblib(dataX_path)
            trainy, _1, valy = utils.read_joblib(datay_path)
            return train_features, val_features, trainy, valy
        if mal_sample_path is None and ben_sample_path is None:
            logger.error("No data. Exit!")
            sys.exit(1)

        mal_feature_list = feature_extraction(mal_sample_path, proc_number=2)
        ben_feature_list = feature_extraction(ben_sample_path, proc_number=2)
        feature_list = mal_feature_list + ben_feature_list
        gt_labels = np.zeros((len(mal_feature_list) + len(ben_feature_list)), dtype=np.int32)
        gt_labels[:len(mal_feature_list)] = 1

        # padding, we neglect the following function and put it in the training pipeline,
        # aiming to save computational memory
        # features = sequence_padding(feature_list,
        #                             conv_win_size= self.hyper_parameters['n_kernel_length'],
        #                             max_seq_length= self.hyper_parameters['max_sequence_length']
        #                             )

        # train-test-validation data splitting
        train_features, test_features, train_y, test_y = \
            train_test_split(feature_list, gt_labels, test_size=0.2, random_state=0)
        train_features, val_features, train_y, val_y = \
            train_test_split(train_features, train_y, test_size=0.25, random_state=0)

        # saving
        utils.dump_joblib([train_features, test_features, val_features], config.get('feature.opcode', 'dataX'))
        utils.dump_joblib([train_y, test_y, val_y], config.get('feature.opcode', 'datay'))

        return train_features, val_features, train_y, val_y

    def _data_standardization(self, featureX, y=None, batch_size=32):
        """
        serialize the data to accommodate the data loader of tf.data.Dataset
        :param featureX: np.array([[list,list,list,...], ... ])
        :param y: ground truth label, if absent, will not be returned
        :return: an object belongs to tf.data.Dataset
        """
        @utils.generator(featureX, y)
        def padding_opcodes(features_of_an_apk,
                            n_kernel_length=self.hyper_parameters['n_kernel_length'],
                            max_seq_length=self.hyper_parameters['max_sequence_length']):
            padding_seq = []
            padding_char = [0] * n_kernel_length
            for i, seq in enumerate(features_of_an_apk):
                padding_seq.extend(seq)
                padding_seq.extend(padding_char)
            return np.array(padding_seq[:max_seq_length])

        # gen_train_dataset = utils.generator
        if y is not None:
            return tf.data.Dataset.from_generator(lambda: padding_opcodes(featureX, self.hyper_parameters['n_kernel_length'], self.hyper_parameters['max_sequence_length']),
                                                  output_types=(tf.int32, tf.int32),
                                                  output_shapes=(tf.TensorShape([None]), tf.TensorShape([]))
                                                  ).cache().shuffle(buffer_size=1000).padded_batch(batch_size,
                                                                                                   padded_shapes=(
                                                                                                       [None], []))
        else:
            return tf.data.Dataset.from_generator(lambda: gen_train_dataset(featureX, y, padding_opcodes),
                                                  output_types=tf.int32,
                                                  output_shapes=tf.TensorShape([None])
                                                  ).cache().shuffle(buffer_size=1000).padded_batch(batch_size,
                                                                                                   padded_shapes=(
                                                                                                   [None]))

    def get_feature_representation(self, apk_paths=None, labels=None, saving_path=None):
        # data preparation
        if saving_path is None:
            saving_path = config.get('feature.opcode', 'test')
        if os.path.exists(saving_path):
            testX, testy = utils.read_joblib(saving_path)
            return testX, testy

        apk_feature_list = []
        gt_labels = []
        min_class_label, max_class_label = np.min(labels), np.max(labels)
        for labels in range(min_class_label, max_class_label + 1, 1):
            flag_label = (np.array(labels) == labels)
            apk_paths_label = np.array(apk_paths)[flag_label].to_list()
            features = feature_extraction(apk_paths_label, proc_number=1)
            apk_feature_list.extend(features)
            gt_labels.extend([labels] * len(features))

        test_features = np.array(apk_feature_list)
        # saving
        utils.dump_joblib((test_features, gt_labels), saving_path)

        return test_features, gt_labels

    def load_test_data(self, apk_paths, gt_labels, feature_saving_path, use_handy_data):
        if use_handy_data:
            _, testX, _ = utils.read_joblib(config.get('feature.opcode', 'dataX'))
            _, gt_labels, _ = utils.read_joblib(config.get('feature.opcode', 'datay'))
        else:
            testX, gt_labels = self.get_feature_representation(apk_paths, gt_labels, feature_saving_path)
        return testX, gt_labels


def _main():
    detector = DeepDroid(model_obj=TextCNN)
    data_path = config.get('dataset', 'dataset_root')
    malware_dir_name = config.get('dataset', 'malware_dir_name')
    benware_dir_name = config.get('dataset', 'benware_dir_name')
    mal_data_path = os.path.join(data_path, malware_dir_name)
    ben_data_path = os.path.join(data_path, benware_dir_name)
    detector.train(mal_data_path, ben_data_path)
    # detector.test()
    return 0


if __name__ == '__main__':
    sys.exit(_main())
